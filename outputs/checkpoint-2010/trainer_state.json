{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9145539906103286,
  "eval_steps": 500,
  "global_step": 1020,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009389671361502348,
      "grad_norm": 0.7170482873916626,
      "learning_rate": 0.00019924953095684804,
      "loss": 0.024366407096385954,
      "step": 5
    },
    {
      "epoch": 0.018779342723004695,
      "grad_norm": 0.5790031552314758,
      "learning_rate": 0.0001983114446529081,
      "loss": 0.029284515976905824,
      "step": 10
    },
    {
      "epoch": 0.028169014084507043,
      "grad_norm": 0.16471530497074127,
      "learning_rate": 0.0001973733583489681,
      "loss": 0.0188706636428833,
      "step": 15
    },
    {
      "epoch": 0.03755868544600939,
      "grad_norm": 0.5223854184150696,
      "learning_rate": 0.00019643527204502817,
      "loss": 0.02484167516231537,
      "step": 20
    },
    {
      "epoch": 0.046948356807511735,
      "grad_norm": 0.5755438804626465,
      "learning_rate": 0.0001954971857410882,
      "loss": 0.024946172535419465,
      "step": 25
    },
    {
      "epoch": 0.056338028169014086,
      "grad_norm": 0.5276082754135132,
      "learning_rate": 0.00019455909943714823,
      "loss": 0.0192352294921875,
      "step": 30
    },
    {
      "epoch": 0.06572769953051644,
      "grad_norm": 0.052270881831645966,
      "learning_rate": 0.00019362101313320827,
      "loss": 0.005262429267168045,
      "step": 35
    },
    {
      "epoch": 0.07511737089201878,
      "grad_norm": 0.43303248286247253,
      "learning_rate": 0.0001926829268292683,
      "loss": 0.032526662945747374,
      "step": 40
    },
    {
      "epoch": 0.08450704225352113,
      "grad_norm": 0.03232896700501442,
      "learning_rate": 0.00019174484052532833,
      "loss": 0.041672608256340025,
      "step": 45
    },
    {
      "epoch": 0.09389671361502347,
      "grad_norm": 0.6141276359558105,
      "learning_rate": 0.00019080675422138838,
      "loss": 0.032822465896606444,
      "step": 50
    },
    {
      "epoch": 0.10328638497652583,
      "grad_norm": 0.4520242214202881,
      "learning_rate": 0.00018986866791744842,
      "loss": 0.04163626730442047,
      "step": 55
    },
    {
      "epoch": 0.11267605633802817,
      "grad_norm": 0.0466199591755867,
      "learning_rate": 0.00018893058161350846,
      "loss": 0.02650299370288849,
      "step": 60
    },
    {
      "epoch": 0.12206572769953052,
      "grad_norm": 0.4180264174938202,
      "learning_rate": 0.00018799249530956848,
      "loss": 0.04091229736804962,
      "step": 65
    },
    {
      "epoch": 0.13145539906103287,
      "grad_norm": 0.3553711771965027,
      "learning_rate": 0.00018705440900562852,
      "loss": 0.015063592791557312,
      "step": 70
    },
    {
      "epoch": 0.14084507042253522,
      "grad_norm": 0.010487482883036137,
      "learning_rate": 0.00018611632270168856,
      "loss": 0.016005079448223113,
      "step": 75
    },
    {
      "epoch": 0.15023474178403756,
      "grad_norm": 0.3133605122566223,
      "learning_rate": 0.0001851782363977486,
      "loss": 0.03900611698627472,
      "step": 80
    },
    {
      "epoch": 0.1596244131455399,
      "grad_norm": 0.2648587226867676,
      "learning_rate": 0.00018424015009380865,
      "loss": 0.03313618302345276,
      "step": 85
    },
    {
      "epoch": 0.16901408450704225,
      "grad_norm": 0.8456673622131348,
      "learning_rate": 0.00018330206378986867,
      "loss": 0.044348093867301944,
      "step": 90
    },
    {
      "epoch": 0.1784037558685446,
      "grad_norm": 0.4668220281600952,
      "learning_rate": 0.0001823639774859287,
      "loss": 0.027705454826354982,
      "step": 95
    },
    {
      "epoch": 0.18779342723004694,
      "grad_norm": 0.39940330386161804,
      "learning_rate": 0.00018142589118198875,
      "loss": 0.020372535288333892,
      "step": 100
    },
    {
      "epoch": 0.19718309859154928,
      "grad_norm": 0.12262050062417984,
      "learning_rate": 0.0001804878048780488,
      "loss": 0.02648935616016388,
      "step": 105
    },
    {
      "epoch": 0.20657276995305165,
      "grad_norm": 0.36662107706069946,
      "learning_rate": 0.00017954971857410884,
      "loss": 0.02500346302986145,
      "step": 110
    },
    {
      "epoch": 0.215962441314554,
      "grad_norm": 0.39640626311302185,
      "learning_rate": 0.00017861163227016885,
      "loss": 0.03066466748714447,
      "step": 115
    },
    {
      "epoch": 0.22535211267605634,
      "grad_norm": 0.13609924912452698,
      "learning_rate": 0.0001776735459662289,
      "loss": 0.034423670172691344,
      "step": 120
    },
    {
      "epoch": 0.2347417840375587,
      "grad_norm": 0.42755258083343506,
      "learning_rate": 0.00017673545966228894,
      "loss": 0.026936334371566773,
      "step": 125
    },
    {
      "epoch": 0.24413145539906103,
      "grad_norm": 0.577203631401062,
      "learning_rate": 0.00017579737335834898,
      "loss": 0.0403815895318985,
      "step": 130
    },
    {
      "epoch": 0.2535211267605634,
      "grad_norm": 0.4630012810230255,
      "learning_rate": 0.00017485928705440902,
      "loss": 0.03422316908836365,
      "step": 135
    },
    {
      "epoch": 0.26291079812206575,
      "grad_norm": 0.44473570585250854,
      "learning_rate": 0.00017392120075046904,
      "loss": 0.04056302309036255,
      "step": 140
    },
    {
      "epoch": 0.27230046948356806,
      "grad_norm": 0.5585605502128601,
      "learning_rate": 0.00017298311444652908,
      "loss": 0.03105192482471466,
      "step": 145
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 0.29612892866134644,
      "learning_rate": 0.00017204502814258913,
      "loss": 0.03291795551776886,
      "step": 150
    },
    {
      "epoch": 0.29107981220657275,
      "grad_norm": 0.32689639925956726,
      "learning_rate": 0.00017110694183864917,
      "loss": 0.02242167741060257,
      "step": 155
    },
    {
      "epoch": 0.3004694835680751,
      "grad_norm": 0.04651407524943352,
      "learning_rate": 0.0001701688555347092,
      "loss": 0.013898816704750062,
      "step": 160
    },
    {
      "epoch": 0.30985915492957744,
      "grad_norm": 0.48786646127700806,
      "learning_rate": 0.00016923076923076923,
      "loss": 0.03659524917602539,
      "step": 165
    },
    {
      "epoch": 0.3192488262910798,
      "grad_norm": 0.5311096906661987,
      "learning_rate": 0.00016829268292682927,
      "loss": 0.02454272210597992,
      "step": 170
    },
    {
      "epoch": 0.3286384976525822,
      "grad_norm": 0.49491727352142334,
      "learning_rate": 0.00016735459662288931,
      "loss": 0.03040455877780914,
      "step": 175
    },
    {
      "epoch": 0.3380281690140845,
      "grad_norm": 0.17964358627796173,
      "learning_rate": 0.00016641651031894936,
      "loss": 0.012564188241958619,
      "step": 180
    },
    {
      "epoch": 0.3474178403755869,
      "grad_norm": 0.43951350450515747,
      "learning_rate": 0.0001654784240150094,
      "loss": 0.03155595362186432,
      "step": 185
    },
    {
      "epoch": 0.3568075117370892,
      "grad_norm": 0.32845795154571533,
      "learning_rate": 0.00016454033771106942,
      "loss": 0.041802632808685306,
      "step": 190
    },
    {
      "epoch": 0.36619718309859156,
      "grad_norm": 0.3292515277862549,
      "learning_rate": 0.00016360225140712946,
      "loss": 0.025441640615463258,
      "step": 195
    },
    {
      "epoch": 0.3755868544600939,
      "grad_norm": 0.23132525384426117,
      "learning_rate": 0.0001626641651031895,
      "loss": 0.017784541845321654,
      "step": 200
    },
    {
      "epoch": 0.38497652582159625,
      "grad_norm": 0.16242405772209167,
      "learning_rate": 0.00016172607879924954,
      "loss": 0.026547151803970336,
      "step": 205
    },
    {
      "epoch": 0.39436619718309857,
      "grad_norm": 0.4249555468559265,
      "learning_rate": 0.0001607879924953096,
      "loss": 0.022331838309764863,
      "step": 210
    },
    {
      "epoch": 0.40375586854460094,
      "grad_norm": 0.5207569599151611,
      "learning_rate": 0.0001598499061913696,
      "loss": 0.03812628984451294,
      "step": 215
    },
    {
      "epoch": 0.4131455399061033,
      "grad_norm": 0.30302780866622925,
      "learning_rate": 0.00015891181988742965,
      "loss": 0.024780957400798796,
      "step": 220
    },
    {
      "epoch": 0.4225352112676056,
      "grad_norm": 0.4814053773880005,
      "learning_rate": 0.0001579737335834897,
      "loss": 0.023501311242580415,
      "step": 225
    },
    {
      "epoch": 0.431924882629108,
      "grad_norm": 0.5907760262489319,
      "learning_rate": 0.00015703564727954973,
      "loss": 0.017664432525634766,
      "step": 230
    },
    {
      "epoch": 0.4413145539906103,
      "grad_norm": 0.708080530166626,
      "learning_rate": 0.00015609756097560978,
      "loss": 0.02111765444278717,
      "step": 235
    },
    {
      "epoch": 0.4507042253521127,
      "grad_norm": 0.40841829776763916,
      "learning_rate": 0.0001551594746716698,
      "loss": 0.026537904143333436,
      "step": 240
    },
    {
      "epoch": 0.460093896713615,
      "grad_norm": 0.5078602433204651,
      "learning_rate": 0.00015422138836772983,
      "loss": 0.029571235179901123,
      "step": 245
    },
    {
      "epoch": 0.4694835680751174,
      "grad_norm": 0.16546864807605743,
      "learning_rate": 0.00015328330206378988,
      "loss": 0.025833728909492492,
      "step": 250
    },
    {
      "epoch": 0.4788732394366197,
      "grad_norm": 0.40888598561286926,
      "learning_rate": 0.00015234521575984992,
      "loss": 0.0066875085234642025,
      "step": 255
    },
    {
      "epoch": 0.48826291079812206,
      "grad_norm": 0.15045152604579926,
      "learning_rate": 0.00015140712945590996,
      "loss": 0.020625768601894377,
      "step": 260
    },
    {
      "epoch": 0.49765258215962443,
      "grad_norm": 0.30756306648254395,
      "learning_rate": 0.00015046904315196998,
      "loss": 0.031203216314315795,
      "step": 265
    },
    {
      "epoch": 0.5070422535211268,
      "grad_norm": 0.37777087092399597,
      "learning_rate": 0.00014953095684803002,
      "loss": 0.026181712746620178,
      "step": 270
    },
    {
      "epoch": 0.5164319248826291,
      "grad_norm": 0.49328917264938354,
      "learning_rate": 0.00014859287054409006,
      "loss": 0.026780527830123902,
      "step": 275
    },
    {
      "epoch": 0.5258215962441315,
      "grad_norm": 0.3769187331199646,
      "learning_rate": 0.00014765478424015008,
      "loss": 0.02104128748178482,
      "step": 280
    },
    {
      "epoch": 0.5352112676056338,
      "grad_norm": 0.07561513036489487,
      "learning_rate": 0.00014671669793621015,
      "loss": 0.021736235916614534,
      "step": 285
    },
    {
      "epoch": 0.5446009389671361,
      "grad_norm": 0.2510596215724945,
      "learning_rate": 0.00014577861163227017,
      "loss": 0.034414997696876524,
      "step": 290
    },
    {
      "epoch": 0.5539906103286385,
      "grad_norm": 0.2250569611787796,
      "learning_rate": 0.0001448405253283302,
      "loss": 0.030404189229011537,
      "step": 295
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 0.295199990272522,
      "learning_rate": 0.00014390243902439025,
      "loss": 0.02633301317691803,
      "step": 300
    },
    {
      "epoch": 0.5727699530516432,
      "grad_norm": 0.7498807311058044,
      "learning_rate": 0.00014296435272045027,
      "loss": 0.04371662735939026,
      "step": 305
    },
    {
      "epoch": 0.5821596244131455,
      "grad_norm": 0.1455170065164566,
      "learning_rate": 0.00014202626641651034,
      "loss": 0.02267677038908005,
      "step": 310
    },
    {
      "epoch": 0.5915492957746479,
      "grad_norm": 0.426584929227829,
      "learning_rate": 0.00014108818011257035,
      "loss": 0.04220994412899017,
      "step": 315
    },
    {
      "epoch": 0.6009389671361502,
      "grad_norm": 0.46855300664901733,
      "learning_rate": 0.0001401500938086304,
      "loss": 0.034361958503723145,
      "step": 320
    },
    {
      "epoch": 0.6103286384976526,
      "grad_norm": 0.40576961636543274,
      "learning_rate": 0.00013921200750469044,
      "loss": 0.03369578421115875,
      "step": 325
    },
    {
      "epoch": 0.6197183098591549,
      "grad_norm": 0.2335013598203659,
      "learning_rate": 0.00013827392120075046,
      "loss": 0.02040947079658508,
      "step": 330
    },
    {
      "epoch": 0.6291079812206573,
      "grad_norm": 0.9166578650474548,
      "learning_rate": 0.00013733583489681053,
      "loss": 0.028532853722572325,
      "step": 335
    },
    {
      "epoch": 0.6384976525821596,
      "grad_norm": 0.31070902943611145,
      "learning_rate": 0.00013639774859287057,
      "loss": 0.024629251658916475,
      "step": 340
    },
    {
      "epoch": 0.647887323943662,
      "grad_norm": 0.22886382043361664,
      "learning_rate": 0.00013545966228893058,
      "loss": 0.01566491723060608,
      "step": 345
    },
    {
      "epoch": 0.6572769953051644,
      "grad_norm": 0.39254266023635864,
      "learning_rate": 0.00013452157598499063,
      "loss": 0.03694182634353638,
      "step": 350
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.21816454827785492,
      "learning_rate": 0.00013358348968105064,
      "loss": 0.01613631844520569,
      "step": 355
    },
    {
      "epoch": 0.676056338028169,
      "grad_norm": 0.3297632932662964,
      "learning_rate": 0.00013264540337711071,
      "loss": 0.01965545117855072,
      "step": 360
    },
    {
      "epoch": 0.6854460093896714,
      "grad_norm": 0.07270976155996323,
      "learning_rate": 0.00013170731707317076,
      "loss": 0.025759166479110716,
      "step": 365
    },
    {
      "epoch": 0.6948356807511737,
      "grad_norm": 0.29896748065948486,
      "learning_rate": 0.00013076923076923077,
      "loss": 0.024330189824104308,
      "step": 370
    },
    {
      "epoch": 0.704225352112676,
      "grad_norm": 0.12587067484855652,
      "learning_rate": 0.00012983114446529082,
      "loss": 0.015517722070217132,
      "step": 375
    },
    {
      "epoch": 0.7136150234741784,
      "grad_norm": 0.10892941057682037,
      "learning_rate": 0.00012889305816135083,
      "loss": 0.008478023856878281,
      "step": 380
    },
    {
      "epoch": 0.7230046948356808,
      "grad_norm": 0.061848726123571396,
      "learning_rate": 0.00012795497185741087,
      "loss": 0.02549907863140106,
      "step": 385
    },
    {
      "epoch": 0.7323943661971831,
      "grad_norm": 0.15808889269828796,
      "learning_rate": 0.00012701688555347094,
      "loss": 0.035325047373771665,
      "step": 390
    },
    {
      "epoch": 0.7417840375586855,
      "grad_norm": 0.2987431287765503,
      "learning_rate": 0.00012607879924953096,
      "loss": 0.027446055412292482,
      "step": 395
    },
    {
      "epoch": 0.7511737089201878,
      "grad_norm": 0.08970658481121063,
      "learning_rate": 0.000125140712945591,
      "loss": 0.026589450240135194,
      "step": 400
    },
    {
      "epoch": 0.7605633802816901,
      "grad_norm": 0.2452840358018875,
      "learning_rate": 0.00012420262664165102,
      "loss": 0.03839884996414185,
      "step": 405
    },
    {
      "epoch": 0.7699530516431925,
      "grad_norm": 0.3062572777271271,
      "learning_rate": 0.00012326454033771106,
      "loss": 0.013943137228488922,
      "step": 410
    },
    {
      "epoch": 0.7793427230046949,
      "grad_norm": 0.1650681048631668,
      "learning_rate": 0.00012232645403377113,
      "loss": 0.028046905994415283,
      "step": 415
    },
    {
      "epoch": 0.7887323943661971,
      "grad_norm": 0.3808073103427887,
      "learning_rate": 0.00012138836772983115,
      "loss": 0.02829594910144806,
      "step": 420
    },
    {
      "epoch": 0.7981220657276995,
      "grad_norm": 0.2380974292755127,
      "learning_rate": 0.00012045028142589119,
      "loss": 0.026149648427963256,
      "step": 425
    },
    {
      "epoch": 0.8075117370892019,
      "grad_norm": 0.29634925723075867,
      "learning_rate": 0.00011951219512195122,
      "loss": 0.021880105137825012,
      "step": 430
    },
    {
      "epoch": 0.8169014084507042,
      "grad_norm": 0.2862769067287445,
      "learning_rate": 0.00011857410881801125,
      "loss": 0.025659793615341188,
      "step": 435
    },
    {
      "epoch": 0.8262910798122066,
      "grad_norm": 0.13496753573417664,
      "learning_rate": 0.0001176360225140713,
      "loss": 0.01359858363866806,
      "step": 440
    },
    {
      "epoch": 0.8356807511737089,
      "grad_norm": 0.4397828280925751,
      "learning_rate": 0.00011669793621013135,
      "loss": 0.030933642387390138,
      "step": 445
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 0.2698576748371124,
      "learning_rate": 0.00011575984990619138,
      "loss": 0.019214542210102083,
      "step": 450
    },
    {
      "epoch": 0.8544600938967136,
      "grad_norm": 0.23861129581928253,
      "learning_rate": 0.00011482176360225141,
      "loss": 0.03139372766017914,
      "step": 455
    },
    {
      "epoch": 0.863849765258216,
      "grad_norm": 0.21139973402023315,
      "learning_rate": 0.00011388367729831144,
      "loss": 0.024115221202373506,
      "step": 460
    },
    {
      "epoch": 0.8732394366197183,
      "grad_norm": 0.1373150795698166,
      "learning_rate": 0.0001129455909943715,
      "loss": 0.02101501077413559,
      "step": 465
    },
    {
      "epoch": 0.8826291079812206,
      "grad_norm": 0.04841241613030434,
      "learning_rate": 0.00011200750469043154,
      "loss": 0.00831463783979416,
      "step": 470
    },
    {
      "epoch": 0.892018779342723,
      "grad_norm": 0.10732517391443253,
      "learning_rate": 0.00011106941838649157,
      "loss": 0.01501830667257309,
      "step": 475
    },
    {
      "epoch": 0.9014084507042254,
      "grad_norm": 0.2256595939397812,
      "learning_rate": 0.0001101313320825516,
      "loss": 0.022538883984088896,
      "step": 480
    },
    {
      "epoch": 0.9107981220657277,
      "grad_norm": 1.3779945373535156,
      "learning_rate": 0.00010919324577861162,
      "loss": 0.028830471634864806,
      "step": 485
    },
    {
      "epoch": 0.92018779342723,
      "grad_norm": 0.5891708731651306,
      "learning_rate": 0.00010825515947467168,
      "loss": 0.036363011598587035,
      "step": 490
    },
    {
      "epoch": 0.9295774647887324,
      "grad_norm": 1.0630677938461304,
      "learning_rate": 0.00010731707317073172,
      "loss": 0.034559860825538635,
      "step": 495
    },
    {
      "epoch": 0.9389671361502347,
      "grad_norm": 0.590244472026825,
      "learning_rate": 0.00010637898686679175,
      "loss": 0.02156972587108612,
      "step": 500
    },
    {
      "epoch": 0.9483568075117371,
      "grad_norm": 0.35359644889831543,
      "learning_rate": 0.00010544090056285178,
      "loss": 0.013540437817573548,
      "step": 505
    },
    {
      "epoch": 0.9577464788732394,
      "grad_norm": 0.5384172797203064,
      "learning_rate": 0.00010450281425891181,
      "loss": 0.018252214789390563,
      "step": 510
    },
    {
      "epoch": 0.9671361502347418,
      "grad_norm": 0.30185315012931824,
      "learning_rate": 0.00010356472795497186,
      "loss": 0.019123917818069457,
      "step": 515
    },
    {
      "epoch": 0.9765258215962441,
      "grad_norm": 0.1674986481666565,
      "learning_rate": 0.00010262664165103191,
      "loss": 0.020727047324180604,
      "step": 520
    },
    {
      "epoch": 0.9859154929577465,
      "grad_norm": 0.14947660267353058,
      "learning_rate": 0.00010168855534709194,
      "loss": 0.03305420875549316,
      "step": 525
    },
    {
      "epoch": 0.9953051643192489,
      "grad_norm": 0.5720767378807068,
      "learning_rate": 0.00010075046904315197,
      "loss": 0.03457537293434143,
      "step": 530
    },
    {
      "epoch": 1.0037558685446009,
      "grad_norm": 0.010408834554255009,
      "learning_rate": 9.981238273921201e-05,
      "loss": 0.022233349084854127,
      "step": 535
    },
    {
      "epoch": 1.0131455399061033,
      "grad_norm": 0.08591393381357193,
      "learning_rate": 9.887429643527206e-05,
      "loss": 0.015992751717567442,
      "step": 540
    },
    {
      "epoch": 1.0225352112676056,
      "grad_norm": 0.25873658061027527,
      "learning_rate": 9.793621013133209e-05,
      "loss": 0.017972575128078462,
      "step": 545
    },
    {
      "epoch": 1.031924882629108,
      "grad_norm": 0.2372436225414276,
      "learning_rate": 9.699812382739213e-05,
      "loss": 0.0181262344121933,
      "step": 550
    },
    {
      "epoch": 1.0413145539906103,
      "grad_norm": 0.06570398062467575,
      "learning_rate": 9.606003752345216e-05,
      "loss": 0.012605157494544984,
      "step": 555
    },
    {
      "epoch": 1.0507042253521126,
      "grad_norm": 0.0045814174227416515,
      "learning_rate": 9.51219512195122e-05,
      "loss": 0.022012335062026978,
      "step": 560
    },
    {
      "epoch": 1.060093896713615,
      "grad_norm": 0.7521730065345764,
      "learning_rate": 9.418386491557224e-05,
      "loss": 0.020697005093097687,
      "step": 565
    },
    {
      "epoch": 1.0694835680751174,
      "grad_norm": 0.16259431838989258,
      "learning_rate": 9.324577861163227e-05,
      "loss": 0.018570949137210847,
      "step": 570
    },
    {
      "epoch": 1.0788732394366196,
      "grad_norm": 0.12314120680093765,
      "learning_rate": 9.230769230769232e-05,
      "loss": 0.019786342978477478,
      "step": 575
    },
    {
      "epoch": 1.088262910798122,
      "grad_norm": 0.28052815794944763,
      "learning_rate": 9.136960600375235e-05,
      "loss": 0.015637752413749696,
      "step": 580
    },
    {
      "epoch": 1.0976525821596244,
      "grad_norm": 0.5417333841323853,
      "learning_rate": 9.043151969981239e-05,
      "loss": 0.03781676590442658,
      "step": 585
    },
    {
      "epoch": 1.1070422535211268,
      "grad_norm": 0.11826757341623306,
      "learning_rate": 8.949343339587243e-05,
      "loss": 0.024798579514026642,
      "step": 590
    },
    {
      "epoch": 1.116431924882629,
      "grad_norm": 0.3249794840812683,
      "learning_rate": 8.855534709193246e-05,
      "loss": 0.023329080641269685,
      "step": 595
    },
    {
      "epoch": 1.1258215962441314,
      "grad_norm": 0.32145726680755615,
      "learning_rate": 8.761726078799249e-05,
      "loss": 0.011117668449878692,
      "step": 600
    },
    {
      "epoch": 1.1352112676056338,
      "grad_norm": 0.3159981966018677,
      "learning_rate": 8.667917448405253e-05,
      "loss": 0.0397039920091629,
      "step": 605
    },
    {
      "epoch": 1.144600938967136,
      "grad_norm": 0.4533611238002777,
      "learning_rate": 8.574108818011258e-05,
      "loss": 0.014920908212661742,
      "step": 610
    },
    {
      "epoch": 1.1539906103286386,
      "grad_norm": 0.005755215417593718,
      "learning_rate": 8.480300187617262e-05,
      "loss": 0.019711731374263762,
      "step": 615
    },
    {
      "epoch": 1.1633802816901408,
      "grad_norm": 0.5304539203643799,
      "learning_rate": 8.386491557223265e-05,
      "loss": 0.026095542311668395,
      "step": 620
    },
    {
      "epoch": 1.172769953051643,
      "grad_norm": 0.33752885460853577,
      "learning_rate": 8.292682926829268e-05,
      "loss": 0.01925521492958069,
      "step": 625
    },
    {
      "epoch": 1.1821596244131456,
      "grad_norm": 0.22369123995304108,
      "learning_rate": 8.198874296435272e-05,
      "loss": 0.024577830731868745,
      "step": 630
    },
    {
      "epoch": 1.1915492957746479,
      "grad_norm": 0.027721889317035675,
      "learning_rate": 8.105065666041276e-05,
      "loss": 0.020502822101116182,
      "step": 635
    },
    {
      "epoch": 1.2009389671361501,
      "grad_norm": 0.8119670152664185,
      "learning_rate": 8.011257035647281e-05,
      "loss": 0.03430028557777405,
      "step": 640
    },
    {
      "epoch": 1.2103286384976526,
      "grad_norm": 0.05463536083698273,
      "learning_rate": 7.917448405253284e-05,
      "loss": 0.007037962228059769,
      "step": 645
    },
    {
      "epoch": 1.2197183098591549,
      "grad_norm": 0.3585096299648285,
      "learning_rate": 7.823639774859287e-05,
      "loss": 0.016571328043937683,
      "step": 650
    },
    {
      "epoch": 1.2291079812206573,
      "grad_norm": 0.044896405190229416,
      "learning_rate": 7.729831144465292e-05,
      "loss": 0.01955685168504715,
      "step": 655
    },
    {
      "epoch": 1.2384976525821596,
      "grad_norm": 0.9038141369819641,
      "learning_rate": 7.636022514071295e-05,
      "loss": 0.012112292647361755,
      "step": 660
    },
    {
      "epoch": 1.247887323943662,
      "grad_norm": 1.3558783531188965,
      "learning_rate": 7.542213883677298e-05,
      "loss": 0.01537751853466034,
      "step": 665
    },
    {
      "epoch": 1.2572769953051643,
      "grad_norm": 0.7927772998809814,
      "learning_rate": 7.448405253283302e-05,
      "loss": 0.015296953916549682,
      "step": 670
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.45680689811706543,
      "learning_rate": 7.354596622889305e-05,
      "loss": 0.012965060770511627,
      "step": 675
    },
    {
      "epoch": 1.276056338028169,
      "grad_norm": 0.447656512260437,
      "learning_rate": 7.260787992495311e-05,
      "loss": 0.017303700745105743,
      "step": 680
    },
    {
      "epoch": 1.2854460093896714,
      "grad_norm": 0.4809512197971344,
      "learning_rate": 7.166979362101314e-05,
      "loss": 0.011106237769126892,
      "step": 685
    },
    {
      "epoch": 1.2948356807511736,
      "grad_norm": 0.10222770273685455,
      "learning_rate": 7.073170731707317e-05,
      "loss": 0.003701509162783623,
      "step": 690
    },
    {
      "epoch": 1.304225352112676,
      "grad_norm": 0.40598100423812866,
      "learning_rate": 6.979362101313321e-05,
      "loss": 0.014276577532291413,
      "step": 695
    },
    {
      "epoch": 1.3136150234741784,
      "grad_norm": 0.9144989848136902,
      "learning_rate": 6.885553470919325e-05,
      "loss": 0.03013971447944641,
      "step": 700
    },
    {
      "epoch": 1.3230046948356806,
      "grad_norm": 0.3137413263320923,
      "learning_rate": 6.791744840525328e-05,
      "loss": 0.018139711022377013,
      "step": 705
    },
    {
      "epoch": 1.332394366197183,
      "grad_norm": 0.8492551445960999,
      "learning_rate": 6.697936210131333e-05,
      "loss": 0.013151729106903076,
      "step": 710
    },
    {
      "epoch": 1.3417840375586856,
      "grad_norm": 0.8693452477455139,
      "learning_rate": 6.604127579737336e-05,
      "loss": 0.023323065042495726,
      "step": 715
    },
    {
      "epoch": 1.3511737089201878,
      "grad_norm": 0.5559377670288086,
      "learning_rate": 6.51031894934334e-05,
      "loss": 0.010653363913297654,
      "step": 720
    },
    {
      "epoch": 1.36056338028169,
      "grad_norm": 0.27534326910972595,
      "learning_rate": 6.416510318949344e-05,
      "loss": 0.006418246775865555,
      "step": 725
    },
    {
      "epoch": 1.3699530516431926,
      "grad_norm": 0.9315496683120728,
      "learning_rate": 6.322701688555347e-05,
      "loss": 0.02736954391002655,
      "step": 730
    },
    {
      "epoch": 1.3793427230046948,
      "grad_norm": 1.51276433467865,
      "learning_rate": 6.228893058161351e-05,
      "loss": 0.014235469698905944,
      "step": 735
    },
    {
      "epoch": 1.388732394366197,
      "grad_norm": 0.7851925492286682,
      "learning_rate": 6.135084427767354e-05,
      "loss": 0.0187423974275589,
      "step": 740
    },
    {
      "epoch": 1.3981220657276996,
      "grad_norm": 0.007494137156754732,
      "learning_rate": 6.0412757973733593e-05,
      "loss": 0.004765984788537026,
      "step": 745
    },
    {
      "epoch": 1.4075117370892019,
      "grad_norm": 0.7774743437767029,
      "learning_rate": 5.947467166979362e-05,
      "loss": 0.014298588037490845,
      "step": 750
    },
    {
      "epoch": 1.4169014084507041,
      "grad_norm": 0.4943155348300934,
      "learning_rate": 5.853658536585366e-05,
      "loss": 0.018778128921985625,
      "step": 755
    },
    {
      "epoch": 1.4262910798122066,
      "grad_norm": 0.6757581233978271,
      "learning_rate": 5.75984990619137e-05,
      "loss": 0.030814680457115173,
      "step": 760
    },
    {
      "epoch": 1.4356807511737089,
      "grad_norm": 1.007874608039856,
      "learning_rate": 5.666041275797374e-05,
      "loss": 0.017760945856571196,
      "step": 765
    },
    {
      "epoch": 1.4450704225352113,
      "grad_norm": 0.27880555391311646,
      "learning_rate": 5.572232645403377e-05,
      "loss": 0.01719447076320648,
      "step": 770
    },
    {
      "epoch": 1.4544600938967136,
      "grad_norm": 0.07802142202854156,
      "learning_rate": 5.478424015009381e-05,
      "loss": 0.016786356270313264,
      "step": 775
    },
    {
      "epoch": 1.463849765258216,
      "grad_norm": 0.33014217019081116,
      "learning_rate": 5.384615384615385e-05,
      "loss": 0.018024437129497528,
      "step": 780
    },
    {
      "epoch": 1.4732394366197183,
      "grad_norm": 0.273423433303833,
      "learning_rate": 5.290806754221389e-05,
      "loss": 0.020227573812007904,
      "step": 785
    },
    {
      "epoch": 1.4826291079812206,
      "grad_norm": 0.6087157726287842,
      "learning_rate": 5.1969981238273926e-05,
      "loss": 0.015724135935306548,
      "step": 790
    },
    {
      "epoch": 1.492018779342723,
      "grad_norm": 0.3836524486541748,
      "learning_rate": 5.1031894934333955e-05,
      "loss": 0.011693578958511353,
      "step": 795
    },
    {
      "epoch": 1.5014084507042254,
      "grad_norm": 0.40421709418296814,
      "learning_rate": 5.0093808630394e-05,
      "loss": 0.01976533830165863,
      "step": 800
    },
    {
      "epoch": 1.5107981220657276,
      "grad_norm": 0.28983503580093384,
      "learning_rate": 4.9155722326454034e-05,
      "loss": 0.006324117630720138,
      "step": 805
    },
    {
      "epoch": 1.52018779342723,
      "grad_norm": 0.9972735643386841,
      "learning_rate": 4.821763602251407e-05,
      "loss": 0.031700342893600464,
      "step": 810
    },
    {
      "epoch": 1.5295774647887324,
      "grad_norm": 0.3925182521343231,
      "learning_rate": 4.727954971857411e-05,
      "loss": 0.006634766608476639,
      "step": 815
    },
    {
      "epoch": 1.5389671361502346,
      "grad_norm": 0.5255478024482727,
      "learning_rate": 4.634146341463415e-05,
      "loss": 0.010036730766296386,
      "step": 820
    },
    {
      "epoch": 1.548356807511737,
      "grad_norm": 0.44627678394317627,
      "learning_rate": 4.5403377110694186e-05,
      "loss": 0.017666324973106384,
      "step": 825
    },
    {
      "epoch": 1.5577464788732396,
      "grad_norm": 0.2978464365005493,
      "learning_rate": 4.446529080675422e-05,
      "loss": 0.007152134180068969,
      "step": 830
    },
    {
      "epoch": 1.5671361502347416,
      "grad_norm": 0.6189464330673218,
      "learning_rate": 4.3527204502814265e-05,
      "loss": 0.01855086088180542,
      "step": 835
    },
    {
      "epoch": 1.576525821596244,
      "grad_norm": 0.2124471515417099,
      "learning_rate": 4.25891181988743e-05,
      "loss": 0.011004482209682465,
      "step": 840
    },
    {
      "epoch": 1.5859154929577466,
      "grad_norm": 1.4865272045135498,
      "learning_rate": 4.165103189493433e-05,
      "loss": 0.029949131608009338,
      "step": 845
    },
    {
      "epoch": 1.5953051643192488,
      "grad_norm": 0.6850019693374634,
      "learning_rate": 4.071294559099437e-05,
      "loss": 0.01705440878868103,
      "step": 850
    },
    {
      "epoch": 1.604694835680751,
      "grad_norm": 0.2630227506160736,
      "learning_rate": 3.977485928705441e-05,
      "loss": 0.009788385033607483,
      "step": 855
    },
    {
      "epoch": 1.6140845070422536,
      "grad_norm": 1.1649662256240845,
      "learning_rate": 3.883677298311445e-05,
      "loss": 0.024737341701984404,
      "step": 860
    },
    {
      "epoch": 1.6234741784037559,
      "grad_norm": 0.10167622566223145,
      "learning_rate": 3.789868667917448e-05,
      "loss": 0.012634846568107604,
      "step": 865
    },
    {
      "epoch": 1.6328638497652581,
      "grad_norm": 0.1363835632801056,
      "learning_rate": 3.6960600375234525e-05,
      "loss": 0.02044594883918762,
      "step": 870
    },
    {
      "epoch": 1.6422535211267606,
      "grad_norm": 0.1351819634437561,
      "learning_rate": 3.602251407129456e-05,
      "loss": 0.006864333152770996,
      "step": 875
    },
    {
      "epoch": 1.6516431924882629,
      "grad_norm": 1.32478666305542,
      "learning_rate": 3.50844277673546e-05,
      "loss": 0.01846112310886383,
      "step": 880
    },
    {
      "epoch": 1.6610328638497651,
      "grad_norm": 0.46798595786094666,
      "learning_rate": 3.414634146341464e-05,
      "loss": 0.0063521571457386015,
      "step": 885
    },
    {
      "epoch": 1.6704225352112676,
      "grad_norm": 0.16015715897083282,
      "learning_rate": 3.320825515947467e-05,
      "loss": 0.016361577808856963,
      "step": 890
    },
    {
      "epoch": 1.67981220657277,
      "grad_norm": 0.4053555428981781,
      "learning_rate": 3.227016885553471e-05,
      "loss": 0.020470038056373596,
      "step": 895
    },
    {
      "epoch": 1.6892018779342723,
      "grad_norm": 0.3714241683483124,
      "learning_rate": 3.133208255159475e-05,
      "loss": 0.0070086784660816194,
      "step": 900
    },
    {
      "epoch": 1.6985915492957746,
      "grad_norm": 0.4469965994358063,
      "learning_rate": 3.0393996247654788e-05,
      "loss": 0.01338874250650406,
      "step": 905
    },
    {
      "epoch": 1.707981220657277,
      "grad_norm": 0.5701423287391663,
      "learning_rate": 2.945590994371482e-05,
      "loss": 0.0326604574918747,
      "step": 910
    },
    {
      "epoch": 1.7173708920187793,
      "grad_norm": 0.9317825436592102,
      "learning_rate": 2.851782363977486e-05,
      "loss": 0.016709166765213012,
      "step": 915
    },
    {
      "epoch": 1.7267605633802816,
      "grad_norm": 0.21620866656303406,
      "learning_rate": 2.75797373358349e-05,
      "loss": 0.01338489055633545,
      "step": 920
    },
    {
      "epoch": 1.736150234741784,
      "grad_norm": 0.08360401540994644,
      "learning_rate": 2.6641651031894936e-05,
      "loss": 0.008626862615346908,
      "step": 925
    },
    {
      "epoch": 1.7455399061032864,
      "grad_norm": 0.03963178023695946,
      "learning_rate": 2.5703564727954972e-05,
      "loss": 0.01531597673892975,
      "step": 930
    },
    {
      "epoch": 1.7549295774647886,
      "grad_norm": 0.5616582036018372,
      "learning_rate": 2.476547842401501e-05,
      "loss": 0.016742998361587526,
      "step": 935
    },
    {
      "epoch": 1.764319248826291,
      "grad_norm": 0.17021504044532776,
      "learning_rate": 2.3827392120075048e-05,
      "loss": 0.0117640882730484,
      "step": 940
    },
    {
      "epoch": 1.7737089201877936,
      "grad_norm": 0.13365834951400757,
      "learning_rate": 2.2889305816135084e-05,
      "loss": 0.0179368793964386,
      "step": 945
    },
    {
      "epoch": 1.7830985915492956,
      "grad_norm": 0.49552884697914124,
      "learning_rate": 2.1951219512195124e-05,
      "loss": 0.026986631751060485,
      "step": 950
    },
    {
      "epoch": 1.792488262910798,
      "grad_norm": 0.3502299189567566,
      "learning_rate": 2.1013133208255163e-05,
      "loss": 0.011717350780963897,
      "step": 955
    },
    {
      "epoch": 1.8018779342723006,
      "grad_norm": 0.20053762197494507,
      "learning_rate": 2.0075046904315196e-05,
      "loss": 0.013132232427597045,
      "step": 960
    },
    {
      "epoch": 1.8112676056338028,
      "grad_norm": 0.34797748923301697,
      "learning_rate": 1.9136960600375236e-05,
      "loss": 0.00402459129691124,
      "step": 965
    },
    {
      "epoch": 1.820657276995305,
      "grad_norm": 0.17338994145393372,
      "learning_rate": 1.8198874296435272e-05,
      "loss": 0.0062800332903862,
      "step": 970
    },
    {
      "epoch": 1.8300469483568076,
      "grad_norm": 0.08744684606790543,
      "learning_rate": 1.726078799249531e-05,
      "loss": 0.008585641533136368,
      "step": 975
    },
    {
      "epoch": 1.8394366197183099,
      "grad_norm": 0.17735736072063446,
      "learning_rate": 1.6322701688555348e-05,
      "loss": 0.01091955006122589,
      "step": 980
    },
    {
      "epoch": 1.8488262910798121,
      "grad_norm": 0.15097256004810333,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 0.012331479787826538,
      "step": 985
    },
    {
      "epoch": 1.8582159624413146,
      "grad_norm": 0.0027024438604712486,
      "learning_rate": 1.4446529080675422e-05,
      "loss": 0.010043661296367645,
      "step": 990
    },
    {
      "epoch": 1.8676056338028169,
      "grad_norm": 0.038844890892505646,
      "learning_rate": 1.3508442776735461e-05,
      "loss": 0.017047320306301118,
      "step": 995
    },
    {
      "epoch": 1.8769953051643191,
      "grad_norm": 0.0183903556317091,
      "learning_rate": 1.2570356472795497e-05,
      "loss": 0.02099301815032959,
      "step": 1000
    },
    {
      "epoch": 1.8863849765258216,
      "grad_norm": 0.4939776659011841,
      "learning_rate": 1.1632270168855535e-05,
      "loss": 0.02314559370279312,
      "step": 1005
    },
    {
      "epoch": 1.895774647887324,
      "grad_norm": 0.708253026008606,
      "learning_rate": 1.0694183864915573e-05,
      "loss": 0.019645790755748748,
      "step": 1010
    },
    {
      "epoch": 1.9051643192488263,
      "grad_norm": 0.914542019367218,
      "learning_rate": 9.756097560975611e-06,
      "loss": 0.026361334323883056,
      "step": 1015
    },
    {
      "epoch": 1.9145539906103286,
      "grad_norm": 0.006782496348023415,
      "learning_rate": 8.818011257035647e-06,
      "loss": 0.010250551998615265,
      "step": 1020
    }
  ],
  "logging_steps": 5,
  "max_steps": 1066,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 20,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1833949195050694e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
