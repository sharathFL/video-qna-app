{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.7830985915492956,
  "eval_steps": 500,
  "global_step": 950,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009389671361502348,
      "grad_norm": 0.9210834503173828,
      "learning_rate": 0.00019924953095684804,
      "loss": 0.1175678014755249,
      "step": 5
    },
    {
      "epoch": 0.018779342723004695,
      "grad_norm": 0.677029550075531,
      "learning_rate": 0.0001983114446529081,
      "loss": 0.12415355443954468,
      "step": 10
    },
    {
      "epoch": 0.028169014084507043,
      "grad_norm": 0.24166812002658844,
      "learning_rate": 0.0001973733583489681,
      "loss": 0.07999475598335266,
      "step": 15
    },
    {
      "epoch": 0.03755868544600939,
      "grad_norm": 0.2671913802623749,
      "learning_rate": 0.00019643527204502817,
      "loss": 0.07308700084686279,
      "step": 20
    },
    {
      "epoch": 0.046948356807511735,
      "grad_norm": 0.1840352714061737,
      "learning_rate": 0.0001954971857410882,
      "loss": 0.06883920431137085,
      "step": 25
    },
    {
      "epoch": 0.056338028169014086,
      "grad_norm": 0.11259204894304276,
      "learning_rate": 0.00019455909943714823,
      "loss": 0.053839457035064694,
      "step": 30
    },
    {
      "epoch": 0.06572769953051644,
      "grad_norm": 0.2519848346710205,
      "learning_rate": 0.00019362101313320827,
      "loss": 0.03236353695392609,
      "step": 35
    },
    {
      "epoch": 0.07511737089201878,
      "grad_norm": 0.19219782948493958,
      "learning_rate": 0.0001926829268292683,
      "loss": 0.08242427110671997,
      "step": 40
    },
    {
      "epoch": 0.08450704225352113,
      "grad_norm": 0.3049354553222656,
      "learning_rate": 0.00019174484052532833,
      "loss": 0.07246894240379334,
      "step": 45
    },
    {
      "epoch": 0.09389671361502347,
      "grad_norm": 0.09659461677074432,
      "learning_rate": 0.00019080675422138838,
      "loss": 0.06766918897628785,
      "step": 50
    },
    {
      "epoch": 0.10328638497652583,
      "grad_norm": 0.21292652189731598,
      "learning_rate": 0.00018986866791744842,
      "loss": 0.07488635182380676,
      "step": 55
    },
    {
      "epoch": 0.11267605633802817,
      "grad_norm": 0.17411686480045319,
      "learning_rate": 0.00018893058161350846,
      "loss": 0.05429755449295044,
      "step": 60
    },
    {
      "epoch": 0.12206572769953052,
      "grad_norm": 0.22193680703639984,
      "learning_rate": 0.00018799249530956848,
      "loss": 0.06525464653968811,
      "step": 65
    },
    {
      "epoch": 0.13145539906103287,
      "grad_norm": 0.22671884298324585,
      "learning_rate": 0.00018705440900562852,
      "loss": 0.047878792881965636,
      "step": 70
    },
    {
      "epoch": 0.14084507042253522,
      "grad_norm": 0.11786852031946182,
      "learning_rate": 0.00018611632270168856,
      "loss": 0.03564092218875885,
      "step": 75
    },
    {
      "epoch": 0.15023474178403756,
      "grad_norm": 0.2878859043121338,
      "learning_rate": 0.0001851782363977486,
      "loss": 0.08285531401634216,
      "step": 80
    },
    {
      "epoch": 0.1596244131455399,
      "grad_norm": 0.09508959949016571,
      "learning_rate": 0.00018424015009380865,
      "loss": 0.059761708974838255,
      "step": 85
    },
    {
      "epoch": 0.16901408450704225,
      "grad_norm": 0.160130575299263,
      "learning_rate": 0.00018330206378986867,
      "loss": 0.062058889865875246,
      "step": 90
    },
    {
      "epoch": 0.1784037558685446,
      "grad_norm": 0.4958449602127075,
      "learning_rate": 0.0001823639774859287,
      "loss": 0.058079713582992555,
      "step": 95
    },
    {
      "epoch": 0.18779342723004694,
      "grad_norm": 0.2689496576786041,
      "learning_rate": 0.00018142589118198875,
      "loss": 0.0604949414730072,
      "step": 100
    },
    {
      "epoch": 0.19718309859154928,
      "grad_norm": 0.17617033421993256,
      "learning_rate": 0.0001804878048780488,
      "loss": 0.06628895998001098,
      "step": 105
    },
    {
      "epoch": 0.20657276995305165,
      "grad_norm": 0.14921841025352478,
      "learning_rate": 0.00017954971857410884,
      "loss": 0.052691173553466794,
      "step": 110
    },
    {
      "epoch": 0.215962441314554,
      "grad_norm": 0.2650579512119293,
      "learning_rate": 0.00017861163227016885,
      "loss": 0.0654792070388794,
      "step": 115
    },
    {
      "epoch": 0.22535211267605634,
      "grad_norm": 0.20644254982471466,
      "learning_rate": 0.0001776735459662289,
      "loss": 0.05628261566162109,
      "step": 120
    },
    {
      "epoch": 0.2347417840375587,
      "grad_norm": 0.12034343183040619,
      "learning_rate": 0.00017673545966228894,
      "loss": 0.050594651699066163,
      "step": 125
    },
    {
      "epoch": 0.24413145539906103,
      "grad_norm": 0.22422173619270325,
      "learning_rate": 0.00017579737335834898,
      "loss": 0.07651615738868714,
      "step": 130
    },
    {
      "epoch": 0.2535211267605634,
      "grad_norm": 0.29449084401130676,
      "learning_rate": 0.00017485928705440902,
      "loss": 0.06289460659027099,
      "step": 135
    },
    {
      "epoch": 0.26291079812206575,
      "grad_norm": 0.17356839776039124,
      "learning_rate": 0.00017392120075046904,
      "loss": 0.04877077043056488,
      "step": 140
    },
    {
      "epoch": 0.27230046948356806,
      "grad_norm": 0.1704159826040268,
      "learning_rate": 0.00017298311444652908,
      "loss": 0.04860035181045532,
      "step": 145
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 0.19696734845638275,
      "learning_rate": 0.00017204502814258913,
      "loss": 0.06895994544029235,
      "step": 150
    },
    {
      "epoch": 0.29107981220657275,
      "grad_norm": 0.10849663615226746,
      "learning_rate": 0.00017110694183864917,
      "loss": 0.04539121389389038,
      "step": 155
    },
    {
      "epoch": 0.3004694835680751,
      "grad_norm": 0.13083192706108093,
      "learning_rate": 0.0001701688555347092,
      "loss": 0.05078960061073303,
      "step": 160
    },
    {
      "epoch": 0.30985915492957744,
      "grad_norm": 0.43957409262657166,
      "learning_rate": 0.00016923076923076923,
      "loss": 0.06918007135391235,
      "step": 165
    },
    {
      "epoch": 0.3192488262910798,
      "grad_norm": 0.22005940973758698,
      "learning_rate": 0.00016829268292682927,
      "loss": 0.061010950803756715,
      "step": 170
    },
    {
      "epoch": 0.3286384976525822,
      "grad_norm": 0.1263442486524582,
      "learning_rate": 0.00016735459662288931,
      "loss": 0.06118847727775574,
      "step": 175
    },
    {
      "epoch": 0.3380281690140845,
      "grad_norm": 0.13231773674488068,
      "learning_rate": 0.00016641651031894936,
      "loss": 0.050911730527877806,
      "step": 180
    },
    {
      "epoch": 0.3474178403755869,
      "grad_norm": 0.10958951711654663,
      "learning_rate": 0.0001654784240150094,
      "loss": 0.046062254905700685,
      "step": 185
    },
    {
      "epoch": 0.3568075117370892,
      "grad_norm": 0.12917718291282654,
      "learning_rate": 0.00016454033771106942,
      "loss": 0.057514244318008424,
      "step": 190
    },
    {
      "epoch": 0.36619718309859156,
      "grad_norm": 0.23355257511138916,
      "learning_rate": 0.00016360225140712946,
      "loss": 0.04758251905441284,
      "step": 195
    },
    {
      "epoch": 0.3755868544600939,
      "grad_norm": 0.163961723446846,
      "learning_rate": 0.0001626641651031895,
      "loss": 0.03815735578536987,
      "step": 200
    },
    {
      "epoch": 0.38497652582159625,
      "grad_norm": 0.1497998982667923,
      "learning_rate": 0.00016172607879924954,
      "loss": 0.047524106502532956,
      "step": 205
    },
    {
      "epoch": 0.39436619718309857,
      "grad_norm": 0.812637448310852,
      "learning_rate": 0.0001607879924953096,
      "loss": 0.03780985474586487,
      "step": 210
    },
    {
      "epoch": 0.40375586854460094,
      "grad_norm": 0.5599747896194458,
      "learning_rate": 0.0001598499061913696,
      "loss": 0.06589300632476806,
      "step": 215
    },
    {
      "epoch": 0.4131455399061033,
      "grad_norm": 0.22932063043117523,
      "learning_rate": 0.00015891181988742965,
      "loss": 0.0419714629650116,
      "step": 220
    },
    {
      "epoch": 0.4225352112676056,
      "grad_norm": 0.1776915341615677,
      "learning_rate": 0.0001579737335834897,
      "loss": 0.05509798526763916,
      "step": 225
    },
    {
      "epoch": 0.431924882629108,
      "grad_norm": 0.31590571999549866,
      "learning_rate": 0.00015703564727954973,
      "loss": 0.042874470353126526,
      "step": 230
    },
    {
      "epoch": 0.4413145539906103,
      "grad_norm": 0.6929092407226562,
      "learning_rate": 0.00015609756097560978,
      "loss": 0.04562438130378723,
      "step": 235
    },
    {
      "epoch": 0.4507042253521127,
      "grad_norm": 0.22109514474868774,
      "learning_rate": 0.0001551594746716698,
      "loss": 0.049407210946083066,
      "step": 240
    },
    {
      "epoch": 0.460093896713615,
      "grad_norm": 0.3618161678314209,
      "learning_rate": 0.00015422138836772983,
      "loss": 0.057310253381729126,
      "step": 245
    },
    {
      "epoch": 0.4694835680751174,
      "grad_norm": 0.21478962898254395,
      "learning_rate": 0.00015328330206378988,
      "loss": 0.06333019137382508,
      "step": 250
    },
    {
      "epoch": 0.4788732394366197,
      "grad_norm": 0.14282406866550446,
      "learning_rate": 0.00015234521575984992,
      "loss": 0.0233868271112442,
      "step": 255
    },
    {
      "epoch": 0.48826291079812206,
      "grad_norm": 0.20286567509174347,
      "learning_rate": 0.00015140712945590996,
      "loss": 0.04580893814563751,
      "step": 260
    },
    {
      "epoch": 0.49765258215962443,
      "grad_norm": 0.26134222745895386,
      "learning_rate": 0.00015046904315196998,
      "loss": 0.06552081108093262,
      "step": 265
    },
    {
      "epoch": 0.5070422535211268,
      "grad_norm": 0.24997009336948395,
      "learning_rate": 0.00014953095684803002,
      "loss": 0.06150721907615662,
      "step": 270
    },
    {
      "epoch": 0.5164319248826291,
      "grad_norm": 0.36435505747795105,
      "learning_rate": 0.00014859287054409006,
      "loss": 0.05370186567306519,
      "step": 275
    },
    {
      "epoch": 0.5258215962441315,
      "grad_norm": 0.4357125759124756,
      "learning_rate": 0.00014765478424015008,
      "loss": 0.044368913769721983,
      "step": 280
    },
    {
      "epoch": 0.5352112676056338,
      "grad_norm": 0.15562988817691803,
      "learning_rate": 0.00014671669793621015,
      "loss": 0.03444035649299622,
      "step": 285
    },
    {
      "epoch": 0.5446009389671361,
      "grad_norm": 0.1602579802274704,
      "learning_rate": 0.00014577861163227017,
      "loss": 0.052959346771240236,
      "step": 290
    },
    {
      "epoch": 0.5539906103286385,
      "grad_norm": 0.2665694057941437,
      "learning_rate": 0.0001448405253283302,
      "loss": 0.03432110846042633,
      "step": 295
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 0.16894084215164185,
      "learning_rate": 0.00014390243902439025,
      "loss": 0.044236835837364194,
      "step": 300
    },
    {
      "epoch": 0.5727699530516432,
      "grad_norm": 0.3137364685535431,
      "learning_rate": 0.00014296435272045027,
      "loss": 0.055343514680862425,
      "step": 305
    },
    {
      "epoch": 0.5821596244131455,
      "grad_norm": 0.14171816408634186,
      "learning_rate": 0.00014202626641651034,
      "loss": 0.03582937717437744,
      "step": 310
    },
    {
      "epoch": 0.5915492957746479,
      "grad_norm": 0.262455552816391,
      "learning_rate": 0.00014108818011257035,
      "loss": 0.05735340118408203,
      "step": 315
    },
    {
      "epoch": 0.6009389671361502,
      "grad_norm": 0.4092520773410797,
      "learning_rate": 0.0001401500938086304,
      "loss": 0.04588189125061035,
      "step": 320
    },
    {
      "epoch": 0.6103286384976526,
      "grad_norm": 0.3626357913017273,
      "learning_rate": 0.00013921200750469044,
      "loss": 0.050349628925323485,
      "step": 325
    },
    {
      "epoch": 0.6197183098591549,
      "grad_norm": 0.16123980283737183,
      "learning_rate": 0.00013827392120075046,
      "loss": 0.036233443021774295,
      "step": 330
    },
    {
      "epoch": 0.6291079812206573,
      "grad_norm": 0.34962916374206543,
      "learning_rate": 0.00013733583489681053,
      "loss": 0.05322943925857544,
      "step": 335
    },
    {
      "epoch": 0.6384976525821596,
      "grad_norm": 0.3084973692893982,
      "learning_rate": 0.00013639774859287057,
      "loss": 0.030023065209388734,
      "step": 340
    },
    {
      "epoch": 0.647887323943662,
      "grad_norm": 0.18579314649105072,
      "learning_rate": 0.00013545966228893058,
      "loss": 0.06129070520401001,
      "step": 345
    },
    {
      "epoch": 0.6572769953051644,
      "grad_norm": 0.30039265751838684,
      "learning_rate": 0.00013452157598499063,
      "loss": 0.04385161995887756,
      "step": 350
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.8118898868560791,
      "learning_rate": 0.00013358348968105064,
      "loss": 0.04234794974327087,
      "step": 355
    },
    {
      "epoch": 0.676056338028169,
      "grad_norm": 0.2992486357688904,
      "learning_rate": 0.00013264540337711071,
      "loss": 0.037520495057106015,
      "step": 360
    },
    {
      "epoch": 0.6854460093896714,
      "grad_norm": 0.25757819414138794,
      "learning_rate": 0.00013170731707317076,
      "loss": 0.04391728937625885,
      "step": 365
    },
    {
      "epoch": 0.6948356807511737,
      "grad_norm": 0.22010010480880737,
      "learning_rate": 0.00013076923076923077,
      "loss": 0.03307545781135559,
      "step": 370
    },
    {
      "epoch": 0.704225352112676,
      "grad_norm": 0.19353753328323364,
      "learning_rate": 0.00012983114446529082,
      "loss": 0.043901649117469785,
      "step": 375
    },
    {
      "epoch": 0.7136150234741784,
      "grad_norm": 0.2031840831041336,
      "learning_rate": 0.00012889305816135083,
      "loss": 0.018644177913665773,
      "step": 380
    },
    {
      "epoch": 0.7230046948356808,
      "grad_norm": 0.2957543432712555,
      "learning_rate": 0.00012795497185741087,
      "loss": 0.035407981276512145,
      "step": 385
    },
    {
      "epoch": 0.7323943661971831,
      "grad_norm": 0.24812796711921692,
      "learning_rate": 0.00012701688555347094,
      "loss": 0.052089530229568484,
      "step": 390
    },
    {
      "epoch": 0.7417840375586855,
      "grad_norm": 0.18746328353881836,
      "learning_rate": 0.00012607879924953096,
      "loss": 0.04592296183109283,
      "step": 395
    },
    {
      "epoch": 0.7511737089201878,
      "grad_norm": 0.2168242633342743,
      "learning_rate": 0.000125140712945591,
      "loss": 0.03811919689178467,
      "step": 400
    },
    {
      "epoch": 0.7605633802816901,
      "grad_norm": 0.2322172373533249,
      "learning_rate": 0.00012420262664165102,
      "loss": 0.05188751220703125,
      "step": 405
    },
    {
      "epoch": 0.7699530516431925,
      "grad_norm": 0.22834369540214539,
      "learning_rate": 0.00012326454033771106,
      "loss": 0.02856787443161011,
      "step": 410
    },
    {
      "epoch": 0.7793427230046949,
      "grad_norm": 0.22223028540611267,
      "learning_rate": 0.00012232645403377113,
      "loss": 0.04182558059692383,
      "step": 415
    },
    {
      "epoch": 0.7887323943661971,
      "grad_norm": 0.37770479917526245,
      "learning_rate": 0.00012138836772983115,
      "loss": 0.03861867189407349,
      "step": 420
    },
    {
      "epoch": 0.7981220657276995,
      "grad_norm": 0.18177230656147003,
      "learning_rate": 0.00012045028142589119,
      "loss": 0.05255597233772278,
      "step": 425
    },
    {
      "epoch": 0.8075117370892019,
      "grad_norm": 0.3399834632873535,
      "learning_rate": 0.00011951219512195122,
      "loss": 0.03605259954929352,
      "step": 430
    },
    {
      "epoch": 0.8169014084507042,
      "grad_norm": 0.42016279697418213,
      "learning_rate": 0.00011857410881801125,
      "loss": 0.038198211789131166,
      "step": 435
    },
    {
      "epoch": 0.8262910798122066,
      "grad_norm": 0.11791759729385376,
      "learning_rate": 0.0001176360225140713,
      "loss": 0.03050377368927002,
      "step": 440
    },
    {
      "epoch": 0.8356807511737089,
      "grad_norm": 0.4137033224105835,
      "learning_rate": 0.00011669793621013135,
      "loss": 0.04679644405841828,
      "step": 445
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 0.2407853901386261,
      "learning_rate": 0.00011575984990619138,
      "loss": 0.03682885468006134,
      "step": 450
    },
    {
      "epoch": 0.8544600938967136,
      "grad_norm": 0.2299448400735855,
      "learning_rate": 0.00011482176360225141,
      "loss": 0.04438296258449555,
      "step": 455
    },
    {
      "epoch": 0.863849765258216,
      "grad_norm": 0.3110993206501007,
      "learning_rate": 0.00011388367729831144,
      "loss": 0.04504597187042236,
      "step": 460
    },
    {
      "epoch": 0.8732394366197183,
      "grad_norm": 0.12201721966266632,
      "learning_rate": 0.0001129455909943715,
      "loss": 0.045019546151161195,
      "step": 465
    },
    {
      "epoch": 0.8826291079812206,
      "grad_norm": 0.2388346940279007,
      "learning_rate": 0.00011200750469043154,
      "loss": 0.029863241314888,
      "step": 470
    },
    {
      "epoch": 0.892018779342723,
      "grad_norm": 0.17069600522518158,
      "learning_rate": 0.00011106941838649157,
      "loss": 0.027168458700180052,
      "step": 475
    },
    {
      "epoch": 0.9014084507042254,
      "grad_norm": 0.21151666343212128,
      "learning_rate": 0.0001101313320825516,
      "loss": 0.032858625054359436,
      "step": 480
    },
    {
      "epoch": 0.9107981220657277,
      "grad_norm": 0.7726204991340637,
      "learning_rate": 0.00010919324577861162,
      "loss": 0.04645529687404633,
      "step": 485
    },
    {
      "epoch": 0.92018779342723,
      "grad_norm": 0.5369940400123596,
      "learning_rate": 0.00010825515947467168,
      "loss": 0.047982001304626466,
      "step": 490
    },
    {
      "epoch": 0.9295774647887324,
      "grad_norm": 0.4152832329273224,
      "learning_rate": 0.00010731707317073172,
      "loss": 0.024782398343086244,
      "step": 495
    },
    {
      "epoch": 0.9389671361502347,
      "grad_norm": 0.20780275762081146,
      "learning_rate": 0.00010637898686679175,
      "loss": 0.026901885867118835,
      "step": 500
    },
    {
      "epoch": 0.9483568075117371,
      "grad_norm": 0.26006951928138733,
      "learning_rate": 0.00010544090056285178,
      "loss": 0.025139790773391724,
      "step": 505
    },
    {
      "epoch": 0.9577464788732394,
      "grad_norm": 0.4392196834087372,
      "learning_rate": 0.00010450281425891181,
      "loss": 0.028442513942718507,
      "step": 510
    },
    {
      "epoch": 0.9671361502347418,
      "grad_norm": 0.24158719182014465,
      "learning_rate": 0.00010356472795497186,
      "loss": 0.0264329731464386,
      "step": 515
    },
    {
      "epoch": 0.9765258215962441,
      "grad_norm": 0.5063230395317078,
      "learning_rate": 0.00010262664165103191,
      "loss": 0.04020188450813293,
      "step": 520
    },
    {
      "epoch": 0.9859154929577465,
      "grad_norm": 0.15892402827739716,
      "learning_rate": 0.00010168855534709194,
      "loss": 0.0455190360546112,
      "step": 525
    },
    {
      "epoch": 0.9953051643192489,
      "grad_norm": 0.6156467795372009,
      "learning_rate": 0.00010075046904315197,
      "loss": 0.05154918432235718,
      "step": 530
    },
    {
      "epoch": 1.0037558685446009,
      "grad_norm": 0.14194010198116302,
      "learning_rate": 9.981238273921201e-05,
      "loss": 0.03189606368541718,
      "step": 535
    },
    {
      "epoch": 1.0131455399061033,
      "grad_norm": 0.1347789764404297,
      "learning_rate": 9.887429643527206e-05,
      "loss": 0.03211231231689453,
      "step": 540
    },
    {
      "epoch": 1.0225352112676056,
      "grad_norm": 0.24436472356319427,
      "learning_rate": 9.793621013133209e-05,
      "loss": 0.03743276000022888,
      "step": 545
    },
    {
      "epoch": 1.031924882629108,
      "grad_norm": 0.26070523262023926,
      "learning_rate": 9.699812382739213e-05,
      "loss": 0.0340953528881073,
      "step": 550
    },
    {
      "epoch": 1.0413145539906103,
      "grad_norm": 0.1581529974937439,
      "learning_rate": 9.606003752345216e-05,
      "loss": 0.021441197395324706,
      "step": 555
    },
    {
      "epoch": 1.0507042253521126,
      "grad_norm": 0.08103559166193008,
      "learning_rate": 9.51219512195122e-05,
      "loss": 0.0341217428445816,
      "step": 560
    },
    {
      "epoch": 1.060093896713615,
      "grad_norm": 0.4079919159412384,
      "learning_rate": 9.418386491557224e-05,
      "loss": 0.02558547556400299,
      "step": 565
    },
    {
      "epoch": 1.0694835680751174,
      "grad_norm": 0.41978973150253296,
      "learning_rate": 9.324577861163227e-05,
      "loss": 0.03545226752758026,
      "step": 570
    },
    {
      "epoch": 1.0788732394366196,
      "grad_norm": 0.26359236240386963,
      "learning_rate": 9.230769230769232e-05,
      "loss": 0.03459569215774536,
      "step": 575
    },
    {
      "epoch": 1.088262910798122,
      "grad_norm": 0.6641039252281189,
      "learning_rate": 9.136960600375235e-05,
      "loss": 0.02265925407409668,
      "step": 580
    },
    {
      "epoch": 1.0976525821596244,
      "grad_norm": 0.281017541885376,
      "learning_rate": 9.043151969981239e-05,
      "loss": 0.03134843707084656,
      "step": 585
    },
    {
      "epoch": 1.1070422535211268,
      "grad_norm": 0.23336221277713776,
      "learning_rate": 8.949343339587243e-05,
      "loss": 0.03373380303382874,
      "step": 590
    },
    {
      "epoch": 1.116431924882629,
      "grad_norm": 0.2950194478034973,
      "learning_rate": 8.855534709193246e-05,
      "loss": 0.03016802966594696,
      "step": 595
    },
    {
      "epoch": 1.1258215962441314,
      "grad_norm": 0.2787869870662689,
      "learning_rate": 8.761726078799249e-05,
      "loss": 0.01817653775215149,
      "step": 600
    },
    {
      "epoch": 1.1352112676056338,
      "grad_norm": 0.08524715155363083,
      "learning_rate": 8.667917448405253e-05,
      "loss": 0.06611374020576477,
      "step": 605
    },
    {
      "epoch": 1.144600938967136,
      "grad_norm": 0.45227381587028503,
      "learning_rate": 8.574108818011258e-05,
      "loss": 0.02642015814781189,
      "step": 610
    },
    {
      "epoch": 1.1539906103286386,
      "grad_norm": 0.0642518699169159,
      "learning_rate": 8.480300187617262e-05,
      "loss": 0.0408567875623703,
      "step": 615
    },
    {
      "epoch": 1.1633802816901408,
      "grad_norm": 0.3047754466533661,
      "learning_rate": 8.386491557223265e-05,
      "loss": 0.04398466944694519,
      "step": 620
    },
    {
      "epoch": 1.172769953051643,
      "grad_norm": 0.2333967238664627,
      "learning_rate": 8.292682926829268e-05,
      "loss": 0.025676196813583373,
      "step": 625
    },
    {
      "epoch": 1.1821596244131456,
      "grad_norm": 0.32625752687454224,
      "learning_rate": 8.198874296435272e-05,
      "loss": 0.0372009813785553,
      "step": 630
    },
    {
      "epoch": 1.1915492957746479,
      "grad_norm": 0.07828962802886963,
      "learning_rate": 8.105065666041276e-05,
      "loss": 0.032852458953857425,
      "step": 635
    },
    {
      "epoch": 1.2009389671361501,
      "grad_norm": 0.3202800750732422,
      "learning_rate": 8.011257035647281e-05,
      "loss": 0.05440183281898499,
      "step": 640
    },
    {
      "epoch": 1.2103286384976526,
      "grad_norm": 0.16760675609111786,
      "learning_rate": 7.917448405253284e-05,
      "loss": 0.018422478437423707,
      "step": 645
    },
    {
      "epoch": 1.2197183098591549,
      "grad_norm": 0.380074679851532,
      "learning_rate": 7.823639774859287e-05,
      "loss": 0.026814153790473937,
      "step": 650
    },
    {
      "epoch": 1.2291079812206573,
      "grad_norm": 0.1862960010766983,
      "learning_rate": 7.729831144465292e-05,
      "loss": 0.037703880667686464,
      "step": 655
    },
    {
      "epoch": 1.2384976525821596,
      "grad_norm": 0.4931422173976898,
      "learning_rate": 7.636022514071295e-05,
      "loss": 0.024777714908123017,
      "step": 660
    },
    {
      "epoch": 1.247887323943662,
      "grad_norm": 0.4385530948638916,
      "learning_rate": 7.542213883677298e-05,
      "loss": 0.03191768527030945,
      "step": 665
    },
    {
      "epoch": 1.2572769953051643,
      "grad_norm": 0.5629450678825378,
      "learning_rate": 7.448405253283302e-05,
      "loss": 0.02685483396053314,
      "step": 670
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.39992761611938477,
      "learning_rate": 7.354596622889305e-05,
      "loss": 0.014876051247119904,
      "step": 675
    },
    {
      "epoch": 1.276056338028169,
      "grad_norm": 0.280097097158432,
      "learning_rate": 7.260787992495311e-05,
      "loss": 0.027761092782020567,
      "step": 680
    },
    {
      "epoch": 1.2854460093896714,
      "grad_norm": 0.629356861114502,
      "learning_rate": 7.166979362101314e-05,
      "loss": 0.02848535180091858,
      "step": 685
    },
    {
      "epoch": 1.2948356807511736,
      "grad_norm": 0.12956301867961884,
      "learning_rate": 7.073170731707317e-05,
      "loss": 0.010111163556575774,
      "step": 690
    },
    {
      "epoch": 1.304225352112676,
      "grad_norm": 0.1482727974653244,
      "learning_rate": 6.979362101313321e-05,
      "loss": 0.018907564878463744,
      "step": 695
    },
    {
      "epoch": 1.3136150234741784,
      "grad_norm": 0.8351060152053833,
      "learning_rate": 6.885553470919325e-05,
      "loss": 0.06243707537651062,
      "step": 700
    },
    {
      "epoch": 1.3230046948356806,
      "grad_norm": 0.31396225094795227,
      "learning_rate": 6.791744840525328e-05,
      "loss": 0.03399330973625183,
      "step": 705
    },
    {
      "epoch": 1.332394366197183,
      "grad_norm": 0.43140292167663574,
      "learning_rate": 6.697936210131333e-05,
      "loss": 0.03371763229370117,
      "step": 710
    },
    {
      "epoch": 1.3417840375586856,
      "grad_norm": 0.42880839109420776,
      "learning_rate": 6.604127579737336e-05,
      "loss": 0.026677751541137697,
      "step": 715
    },
    {
      "epoch": 1.3511737089201878,
      "grad_norm": 0.35171228647232056,
      "learning_rate": 6.51031894934334e-05,
      "loss": 0.031094890832901,
      "step": 720
    },
    {
      "epoch": 1.36056338028169,
      "grad_norm": 0.40206584334373474,
      "learning_rate": 6.416510318949344e-05,
      "loss": 0.016980856657028198,
      "step": 725
    },
    {
      "epoch": 1.3699530516431926,
      "grad_norm": 0.7395554184913635,
      "learning_rate": 6.322701688555347e-05,
      "loss": 0.05275000929832459,
      "step": 730
    },
    {
      "epoch": 1.3793427230046948,
      "grad_norm": 0.33429762721061707,
      "learning_rate": 6.228893058161351e-05,
      "loss": 0.02763172686100006,
      "step": 735
    },
    {
      "epoch": 1.388732394366197,
      "grad_norm": 0.3986763656139374,
      "learning_rate": 6.135084427767354e-05,
      "loss": 0.03092779517173767,
      "step": 740
    },
    {
      "epoch": 1.3981220657276996,
      "grad_norm": 0.16678722202777863,
      "learning_rate": 6.0412757973733593e-05,
      "loss": 0.017231525480747224,
      "step": 745
    },
    {
      "epoch": 1.4075117370892019,
      "grad_norm": 0.4162488877773285,
      "learning_rate": 5.947467166979362e-05,
      "loss": 0.023038730025291443,
      "step": 750
    },
    {
      "epoch": 1.4169014084507041,
      "grad_norm": 0.26193615794181824,
      "learning_rate": 5.853658536585366e-05,
      "loss": 0.03223239481449127,
      "step": 755
    },
    {
      "epoch": 1.4262910798122066,
      "grad_norm": 0.20061108469963074,
      "learning_rate": 5.75984990619137e-05,
      "loss": 0.04780903458595276,
      "step": 760
    },
    {
      "epoch": 1.4356807511737089,
      "grad_norm": 0.3794776201248169,
      "learning_rate": 5.666041275797374e-05,
      "loss": 0.021070212125778198,
      "step": 765
    },
    {
      "epoch": 1.4450704225352113,
      "grad_norm": 0.27470335364341736,
      "learning_rate": 5.572232645403377e-05,
      "loss": 0.025725087523460387,
      "step": 770
    },
    {
      "epoch": 1.4544600938967136,
      "grad_norm": 0.12825986742973328,
      "learning_rate": 5.478424015009381e-05,
      "loss": 0.02595835328102112,
      "step": 775
    },
    {
      "epoch": 1.463849765258216,
      "grad_norm": 0.5213990807533264,
      "learning_rate": 5.384615384615385e-05,
      "loss": 0.039664891362190244,
      "step": 780
    },
    {
      "epoch": 1.4732394366197183,
      "grad_norm": 0.5635453462600708,
      "learning_rate": 5.290806754221389e-05,
      "loss": 0.03409115672111511,
      "step": 785
    },
    {
      "epoch": 1.4826291079812206,
      "grad_norm": 0.4034866392612457,
      "learning_rate": 5.1969981238273926e-05,
      "loss": 0.030914965271949767,
      "step": 790
    },
    {
      "epoch": 1.492018779342723,
      "grad_norm": 0.42897966504096985,
      "learning_rate": 5.1031894934333955e-05,
      "loss": 0.030643028020858765,
      "step": 795
    },
    {
      "epoch": 1.5014084507042254,
      "grad_norm": 0.37180662155151367,
      "learning_rate": 5.0093808630394e-05,
      "loss": 0.03262801766395569,
      "step": 800
    },
    {
      "epoch": 1.5107981220657276,
      "grad_norm": 0.4208682179450989,
      "learning_rate": 4.9155722326454034e-05,
      "loss": 0.02157711237668991,
      "step": 805
    },
    {
      "epoch": 1.52018779342723,
      "grad_norm": 0.6487414836883545,
      "learning_rate": 4.821763602251407e-05,
      "loss": 0.044314262270927426,
      "step": 810
    },
    {
      "epoch": 1.5295774647887324,
      "grad_norm": 0.3399123549461365,
      "learning_rate": 4.727954971857411e-05,
      "loss": 0.0207409605383873,
      "step": 815
    },
    {
      "epoch": 1.5389671361502346,
      "grad_norm": 0.2230329066514969,
      "learning_rate": 4.634146341463415e-05,
      "loss": 0.022801519930362703,
      "step": 820
    },
    {
      "epoch": 1.548356807511737,
      "grad_norm": 0.21257899701595306,
      "learning_rate": 4.5403377110694186e-05,
      "loss": 0.021042965352535248,
      "step": 825
    },
    {
      "epoch": 1.5577464788732396,
      "grad_norm": 0.37352922558784485,
      "learning_rate": 4.446529080675422e-05,
      "loss": 0.015470266342163086,
      "step": 830
    },
    {
      "epoch": 1.5671361502347416,
      "grad_norm": 0.43816128373146057,
      "learning_rate": 4.3527204502814265e-05,
      "loss": 0.024192583560943604,
      "step": 835
    },
    {
      "epoch": 1.576525821596244,
      "grad_norm": 0.24930895864963531,
      "learning_rate": 4.25891181988743e-05,
      "loss": 0.023281671106815338,
      "step": 840
    },
    {
      "epoch": 1.5859154929577466,
      "grad_norm": 0.8968409299850464,
      "learning_rate": 4.165103189493433e-05,
      "loss": 0.02439534366130829,
      "step": 845
    },
    {
      "epoch": 1.5953051643192488,
      "grad_norm": 0.4216288924217224,
      "learning_rate": 4.071294559099437e-05,
      "loss": 0.0259724497795105,
      "step": 850
    },
    {
      "epoch": 1.604694835680751,
      "grad_norm": 0.3559226393699646,
      "learning_rate": 3.977485928705441e-05,
      "loss": 0.01730057895183563,
      "step": 855
    },
    {
      "epoch": 1.6140845070422536,
      "grad_norm": 0.8114773631095886,
      "learning_rate": 3.883677298311445e-05,
      "loss": 0.025870645046234132,
      "step": 860
    },
    {
      "epoch": 1.6234741784037559,
      "grad_norm": 0.30473506450653076,
      "learning_rate": 3.789868667917448e-05,
      "loss": 0.028008559346199037,
      "step": 865
    },
    {
      "epoch": 1.6328638497652581,
      "grad_norm": 0.20021593570709229,
      "learning_rate": 3.6960600375234525e-05,
      "loss": 0.036058041453361514,
      "step": 870
    },
    {
      "epoch": 1.6422535211267606,
      "grad_norm": 0.37913069128990173,
      "learning_rate": 3.602251407129456e-05,
      "loss": 0.018290480971336363,
      "step": 875
    },
    {
      "epoch": 1.6516431924882629,
      "grad_norm": 0.5045548677444458,
      "learning_rate": 3.50844277673546e-05,
      "loss": 0.025209194421768187,
      "step": 880
    },
    {
      "epoch": 1.6610328638497651,
      "grad_norm": 0.2946089208126068,
      "learning_rate": 3.414634146341464e-05,
      "loss": 0.012755483388900757,
      "step": 885
    },
    {
      "epoch": 1.6704225352112676,
      "grad_norm": 0.30867812037467957,
      "learning_rate": 3.320825515947467e-05,
      "loss": 0.031549885869026184,
      "step": 890
    },
    {
      "epoch": 1.67981220657277,
      "grad_norm": 0.32761257886886597,
      "learning_rate": 3.227016885553471e-05,
      "loss": 0.03701130151748657,
      "step": 895
    },
    {
      "epoch": 1.6892018779342723,
      "grad_norm": 0.267748087644577,
      "learning_rate": 3.133208255159475e-05,
      "loss": 0.015203697979450226,
      "step": 900
    },
    {
      "epoch": 1.6985915492957746,
      "grad_norm": 0.6596513390541077,
      "learning_rate": 3.0393996247654788e-05,
      "loss": 0.024025607109069824,
      "step": 905
    },
    {
      "epoch": 1.707981220657277,
      "grad_norm": 0.5584546327590942,
      "learning_rate": 2.945590994371482e-05,
      "loss": 0.04370039701461792,
      "step": 910
    },
    {
      "epoch": 1.7173708920187793,
      "grad_norm": 0.48765286803245544,
      "learning_rate": 2.851782363977486e-05,
      "loss": 0.019050830602645875,
      "step": 915
    },
    {
      "epoch": 1.7267605633802816,
      "grad_norm": 1.0972315073013306,
      "learning_rate": 2.75797373358349e-05,
      "loss": 0.03419985175132752,
      "step": 920
    },
    {
      "epoch": 1.736150234741784,
      "grad_norm": 0.11108722537755966,
      "learning_rate": 2.6641651031894936e-05,
      "loss": 0.021352480351924896,
      "step": 925
    },
    {
      "epoch": 1.7455399061032864,
      "grad_norm": 0.12393585592508316,
      "learning_rate": 2.5703564727954972e-05,
      "loss": 0.02515890300273895,
      "step": 930
    },
    {
      "epoch": 1.7549295774647886,
      "grad_norm": 0.4569076895713806,
      "learning_rate": 2.476547842401501e-05,
      "loss": 0.03458903133869171,
      "step": 935
    },
    {
      "epoch": 1.764319248826291,
      "grad_norm": 0.2247631698846817,
      "learning_rate": 2.3827392120075048e-05,
      "loss": 0.02864217758178711,
      "step": 940
    },
    {
      "epoch": 1.7737089201877936,
      "grad_norm": 0.046243857592344284,
      "learning_rate": 2.2889305816135084e-05,
      "loss": 0.016363075375556944,
      "step": 945
    },
    {
      "epoch": 1.7830985915492956,
      "grad_norm": 0.3749868869781494,
      "learning_rate": 2.1951219512195124e-05,
      "loss": 0.03056669533252716,
      "step": 950
    }
  ],
  "logging_steps": 5,
  "max_steps": 1066,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1021414452255008e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
