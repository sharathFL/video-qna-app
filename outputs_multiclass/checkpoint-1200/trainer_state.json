{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.8964426877470357,
  "eval_steps": 500,
  "global_step": 1200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007905138339920948,
      "grad_norm": 1.6806895732879639,
      "learning_rate": 0.00019936808846761453,
      "loss": 1.8635698318481446,
      "step": 5
    },
    {
      "epoch": 0.015810276679841896,
      "grad_norm": 0.9938549995422363,
      "learning_rate": 0.0001985781990521327,
      "loss": 0.9470885276794434,
      "step": 10
    },
    {
      "epoch": 0.023715415019762844,
      "grad_norm": 0.7372719645500183,
      "learning_rate": 0.00019778830963665086,
      "loss": 0.3602733135223389,
      "step": 15
    },
    {
      "epoch": 0.03162055335968379,
      "grad_norm": 0.32483407855033875,
      "learning_rate": 0.00019699842022116905,
      "loss": 0.13549641370773316,
      "step": 20
    },
    {
      "epoch": 0.039525691699604744,
      "grad_norm": 0.40757089853286743,
      "learning_rate": 0.0001962085308056872,
      "loss": 0.07350534200668335,
      "step": 25
    },
    {
      "epoch": 0.04743083003952569,
      "grad_norm": 0.40076902508735657,
      "learning_rate": 0.00019541864139020537,
      "loss": 0.0714210569858551,
      "step": 30
    },
    {
      "epoch": 0.05533596837944664,
      "grad_norm": 0.291513055562973,
      "learning_rate": 0.00019462875197472353,
      "loss": 0.06504204869270325,
      "step": 35
    },
    {
      "epoch": 0.06324110671936758,
      "grad_norm": 0.20724964141845703,
      "learning_rate": 0.00019383886255924172,
      "loss": 0.05190123319625854,
      "step": 40
    },
    {
      "epoch": 0.07114624505928854,
      "grad_norm": 0.13631896674633026,
      "learning_rate": 0.00019304897314375988,
      "loss": 0.056236618757247926,
      "step": 45
    },
    {
      "epoch": 0.07905138339920949,
      "grad_norm": 0.17905187606811523,
      "learning_rate": 0.00019225908372827804,
      "loss": 0.06204631328582764,
      "step": 50
    },
    {
      "epoch": 0.08695652173913043,
      "grad_norm": 0.20501917600631714,
      "learning_rate": 0.0001914691943127962,
      "loss": 0.05558158159255981,
      "step": 55
    },
    {
      "epoch": 0.09486166007905138,
      "grad_norm": 0.2663598358631134,
      "learning_rate": 0.00019067930489731437,
      "loss": 0.04937282502651215,
      "step": 60
    },
    {
      "epoch": 0.10276679841897234,
      "grad_norm": 0.3612702488899231,
      "learning_rate": 0.00018988941548183256,
      "loss": 0.060613608360290526,
      "step": 65
    },
    {
      "epoch": 0.11067193675889328,
      "grad_norm": 0.2817140221595764,
      "learning_rate": 0.00018909952606635072,
      "loss": 0.051828396320343015,
      "step": 70
    },
    {
      "epoch": 0.11857707509881422,
      "grad_norm": 0.20045047998428345,
      "learning_rate": 0.00018830963665086888,
      "loss": 0.04578936696052551,
      "step": 75
    },
    {
      "epoch": 0.12648221343873517,
      "grad_norm": 0.1883169412612915,
      "learning_rate": 0.00018751974723538704,
      "loss": 0.04504136145114899,
      "step": 80
    },
    {
      "epoch": 0.13438735177865613,
      "grad_norm": 0.18971523642539978,
      "learning_rate": 0.00018672985781990523,
      "loss": 0.04297415912151337,
      "step": 85
    },
    {
      "epoch": 0.1422924901185771,
      "grad_norm": 0.16875876486301422,
      "learning_rate": 0.0001859399684044234,
      "loss": 0.06560943126678467,
      "step": 90
    },
    {
      "epoch": 0.15019762845849802,
      "grad_norm": 0.15045519173145294,
      "learning_rate": 0.00018515007898894156,
      "loss": 0.04345881640911102,
      "step": 95
    },
    {
      "epoch": 0.15810276679841898,
      "grad_norm": 0.23051920533180237,
      "learning_rate": 0.00018436018957345972,
      "loss": 0.04645131826400757,
      "step": 100
    },
    {
      "epoch": 0.16600790513833993,
      "grad_norm": 0.43637263774871826,
      "learning_rate": 0.0001835703001579779,
      "loss": 0.05234196186065674,
      "step": 105
    },
    {
      "epoch": 0.17391304347826086,
      "grad_norm": 0.14255349338054657,
      "learning_rate": 0.00018278041074249607,
      "loss": 0.03945432305335998,
      "step": 110
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.9225562810897827,
      "learning_rate": 0.00018199052132701423,
      "loss": 0.06067950129508972,
      "step": 115
    },
    {
      "epoch": 0.18972332015810275,
      "grad_norm": 0.1776546686887741,
      "learning_rate": 0.0001812006319115324,
      "loss": 0.05468782186508179,
      "step": 120
    },
    {
      "epoch": 0.1976284584980237,
      "grad_norm": 0.1758536994457245,
      "learning_rate": 0.00018041074249605055,
      "loss": 0.04529163539409638,
      "step": 125
    },
    {
      "epoch": 0.20553359683794467,
      "grad_norm": 0.12450053542852402,
      "learning_rate": 0.00017962085308056874,
      "loss": 0.049734491109848025,
      "step": 130
    },
    {
      "epoch": 0.2134387351778656,
      "grad_norm": 0.34710755944252014,
      "learning_rate": 0.0001788309636650869,
      "loss": 0.048593145608901975,
      "step": 135
    },
    {
      "epoch": 0.22134387351778656,
      "grad_norm": 0.31041809916496277,
      "learning_rate": 0.00017804107424960507,
      "loss": 0.04384960532188416,
      "step": 140
    },
    {
      "epoch": 0.22924901185770752,
      "grad_norm": 0.15124908089637756,
      "learning_rate": 0.00017725118483412323,
      "loss": 0.034817859530448914,
      "step": 145
    },
    {
      "epoch": 0.23715415019762845,
      "grad_norm": 0.11687161773443222,
      "learning_rate": 0.00017646129541864142,
      "loss": 0.0549238383769989,
      "step": 150
    },
    {
      "epoch": 0.2450592885375494,
      "grad_norm": 0.09150560200214386,
      "learning_rate": 0.00017567140600315958,
      "loss": 0.04390700459480286,
      "step": 155
    },
    {
      "epoch": 0.25296442687747034,
      "grad_norm": 0.1208692416548729,
      "learning_rate": 0.00017488151658767774,
      "loss": 0.046901670098304746,
      "step": 160
    },
    {
      "epoch": 0.2608695652173913,
      "grad_norm": 0.14429397881031036,
      "learning_rate": 0.0001740916271721959,
      "loss": 0.041185769438743594,
      "step": 165
    },
    {
      "epoch": 0.26877470355731226,
      "grad_norm": 0.26194989681243896,
      "learning_rate": 0.0001733017377567141,
      "loss": 0.04512665867805481,
      "step": 170
    },
    {
      "epoch": 0.2766798418972332,
      "grad_norm": 0.12198891490697861,
      "learning_rate": 0.00017251184834123225,
      "loss": 0.046025010943412784,
      "step": 175
    },
    {
      "epoch": 0.2845849802371542,
      "grad_norm": 0.1712380051612854,
      "learning_rate": 0.00017172195892575041,
      "loss": 0.04504755139350891,
      "step": 180
    },
    {
      "epoch": 0.2924901185770751,
      "grad_norm": 0.16783832013607025,
      "learning_rate": 0.00017093206951026858,
      "loss": 0.04138432145118713,
      "step": 185
    },
    {
      "epoch": 0.30039525691699603,
      "grad_norm": 0.21837900578975677,
      "learning_rate": 0.00017014218009478674,
      "loss": 0.03247830867767334,
      "step": 190
    },
    {
      "epoch": 0.308300395256917,
      "grad_norm": 0.1421031355857849,
      "learning_rate": 0.00016935229067930493,
      "loss": 0.04860071837902069,
      "step": 195
    },
    {
      "epoch": 0.31620553359683795,
      "grad_norm": 0.20984382927417755,
      "learning_rate": 0.0001685624012638231,
      "loss": 0.04359340071678162,
      "step": 200
    },
    {
      "epoch": 0.3241106719367589,
      "grad_norm": 0.28956568241119385,
      "learning_rate": 0.00016777251184834125,
      "loss": 0.04692917168140411,
      "step": 205
    },
    {
      "epoch": 0.33201581027667987,
      "grad_norm": 0.26215651631355286,
      "learning_rate": 0.0001669826224328594,
      "loss": 0.04435140192508698,
      "step": 210
    },
    {
      "epoch": 0.33992094861660077,
      "grad_norm": 0.15929624438285828,
      "learning_rate": 0.00016619273301737757,
      "loss": 0.0358163595199585,
      "step": 215
    },
    {
      "epoch": 0.34782608695652173,
      "grad_norm": 0.10788040608167648,
      "learning_rate": 0.00016540284360189576,
      "loss": 0.03285063207149506,
      "step": 220
    },
    {
      "epoch": 0.3557312252964427,
      "grad_norm": 0.13274972140789032,
      "learning_rate": 0.00016461295418641393,
      "loss": 0.03898800015449524,
      "step": 225
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.3682556748390198,
      "learning_rate": 0.0001638230647709321,
      "loss": 0.03944840431213379,
      "step": 230
    },
    {
      "epoch": 0.3715415019762846,
      "grad_norm": 0.25130292773246765,
      "learning_rate": 0.00016303317535545025,
      "loss": 0.04706685245037079,
      "step": 235
    },
    {
      "epoch": 0.3794466403162055,
      "grad_norm": 0.2393902838230133,
      "learning_rate": 0.0001622432859399684,
      "loss": 0.038498157262802125,
      "step": 240
    },
    {
      "epoch": 0.38735177865612647,
      "grad_norm": 0.17944279313087463,
      "learning_rate": 0.0001614533965244866,
      "loss": 0.0442039281129837,
      "step": 245
    },
    {
      "epoch": 0.3952569169960474,
      "grad_norm": 0.14270934462547302,
      "learning_rate": 0.00016066350710900476,
      "loss": 0.045394104719161985,
      "step": 250
    },
    {
      "epoch": 0.4031620553359684,
      "grad_norm": 0.11706896126270294,
      "learning_rate": 0.00015987361769352292,
      "loss": 0.04451458156108856,
      "step": 255
    },
    {
      "epoch": 0.41106719367588934,
      "grad_norm": 0.22343292832374573,
      "learning_rate": 0.00015908372827804109,
      "loss": 0.04224025309085846,
      "step": 260
    },
    {
      "epoch": 0.4189723320158103,
      "grad_norm": 0.150027796626091,
      "learning_rate": 0.00015829383886255925,
      "loss": 0.04553111791610718,
      "step": 265
    },
    {
      "epoch": 0.4268774703557312,
      "grad_norm": 0.19180583953857422,
      "learning_rate": 0.00015750394944707744,
      "loss": 0.046068915724754335,
      "step": 270
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 0.3550284504890442,
      "learning_rate": 0.0001567140600315956,
      "loss": 0.043608084321022034,
      "step": 275
    },
    {
      "epoch": 0.4426877470355731,
      "grad_norm": 0.1059073954820633,
      "learning_rate": 0.00015592417061611376,
      "loss": 0.028976786136627197,
      "step": 280
    },
    {
      "epoch": 0.4505928853754941,
      "grad_norm": 0.11095951497554779,
      "learning_rate": 0.00015513428120063192,
      "loss": 0.027904754877090453,
      "step": 285
    },
    {
      "epoch": 0.45849802371541504,
      "grad_norm": 0.1516638696193695,
      "learning_rate": 0.00015434439178515008,
      "loss": 0.03798638880252838,
      "step": 290
    },
    {
      "epoch": 0.466403162055336,
      "grad_norm": 0.35330235958099365,
      "learning_rate": 0.00015355450236966827,
      "loss": 0.038866636157035825,
      "step": 295
    },
    {
      "epoch": 0.4743083003952569,
      "grad_norm": 0.3178061246871948,
      "learning_rate": 0.00015276461295418643,
      "loss": 0.03945719599723816,
      "step": 300
    },
    {
      "epoch": 0.48221343873517786,
      "grad_norm": 0.2744901776313782,
      "learning_rate": 0.0001519747235387046,
      "loss": 0.03833763599395752,
      "step": 305
    },
    {
      "epoch": 0.4901185770750988,
      "grad_norm": 0.1693403124809265,
      "learning_rate": 0.00015118483412322276,
      "loss": 0.03069846034049988,
      "step": 310
    },
    {
      "epoch": 0.4980237154150198,
      "grad_norm": 0.3653373420238495,
      "learning_rate": 0.00015039494470774092,
      "loss": 0.042437827587127684,
      "step": 315
    },
    {
      "epoch": 0.5059288537549407,
      "grad_norm": 0.18451716005802155,
      "learning_rate": 0.0001496050552922591,
      "loss": 0.04567863941192627,
      "step": 320
    },
    {
      "epoch": 0.5138339920948617,
      "grad_norm": 0.18308185040950775,
      "learning_rate": 0.00014881516587677727,
      "loss": 0.04377282559871674,
      "step": 325
    },
    {
      "epoch": 0.5217391304347826,
      "grad_norm": 0.16262520849704742,
      "learning_rate": 0.00014802527646129543,
      "loss": 0.03364770710468292,
      "step": 330
    },
    {
      "epoch": 0.5296442687747036,
      "grad_norm": 0.2911263108253479,
      "learning_rate": 0.0001472353870458136,
      "loss": 0.044137999415397644,
      "step": 335
    },
    {
      "epoch": 0.5375494071146245,
      "grad_norm": 0.14540807902812958,
      "learning_rate": 0.00014644549763033176,
      "loss": 0.039713618159294126,
      "step": 340
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.15037724375724792,
      "learning_rate": 0.00014565560821484994,
      "loss": 0.041572985053062436,
      "step": 345
    },
    {
      "epoch": 0.5533596837944664,
      "grad_norm": 0.25399377942085266,
      "learning_rate": 0.0001448657187993681,
      "loss": 0.035783502459526065,
      "step": 350
    },
    {
      "epoch": 0.5612648221343873,
      "grad_norm": 0.24875879287719727,
      "learning_rate": 0.00014407582938388627,
      "loss": 0.03359948992729187,
      "step": 355
    },
    {
      "epoch": 0.5691699604743083,
      "grad_norm": 1.0093599557876587,
      "learning_rate": 0.00014328593996840443,
      "loss": 0.039142918586730954,
      "step": 360
    },
    {
      "epoch": 0.5770750988142292,
      "grad_norm": 0.1824008673429489,
      "learning_rate": 0.0001424960505529226,
      "loss": 0.029236820340156556,
      "step": 365
    },
    {
      "epoch": 0.5849802371541502,
      "grad_norm": 0.7013839483261108,
      "learning_rate": 0.00014170616113744078,
      "loss": 0.0378925234079361,
      "step": 370
    },
    {
      "epoch": 0.5928853754940712,
      "grad_norm": 0.2092132419347763,
      "learning_rate": 0.00014091627172195894,
      "loss": 0.048207801580429074,
      "step": 375
    },
    {
      "epoch": 0.6007905138339921,
      "grad_norm": 0.3133586049079895,
      "learning_rate": 0.0001401263823064771,
      "loss": 0.031995180249214175,
      "step": 380
    },
    {
      "epoch": 0.6086956521739131,
      "grad_norm": 0.2936135530471802,
      "learning_rate": 0.00013933649289099527,
      "loss": 0.04239987134933472,
      "step": 385
    },
    {
      "epoch": 0.616600790513834,
      "grad_norm": 0.28508004546165466,
      "learning_rate": 0.00013854660347551343,
      "loss": 0.04000416398048401,
      "step": 390
    },
    {
      "epoch": 0.6245059288537549,
      "grad_norm": 0.1706475466489792,
      "learning_rate": 0.00013775671406003162,
      "loss": 0.03937291800975799,
      "step": 395
    },
    {
      "epoch": 0.6324110671936759,
      "grad_norm": 0.20164187252521515,
      "learning_rate": 0.00013696682464454978,
      "loss": 0.0373034805059433,
      "step": 400
    },
    {
      "epoch": 0.6403162055335968,
      "grad_norm": 0.11471879482269287,
      "learning_rate": 0.00013617693522906794,
      "loss": 0.03275639116764069,
      "step": 405
    },
    {
      "epoch": 0.6482213438735178,
      "grad_norm": 0.183430477976799,
      "learning_rate": 0.0001353870458135861,
      "loss": 0.03503856658935547,
      "step": 410
    },
    {
      "epoch": 0.6561264822134387,
      "grad_norm": 0.15381184220314026,
      "learning_rate": 0.00013459715639810426,
      "loss": 0.04328701198101044,
      "step": 415
    },
    {
      "epoch": 0.6640316205533597,
      "grad_norm": 0.17136786878108978,
      "learning_rate": 0.00013380726698262245,
      "loss": 0.04007150828838348,
      "step": 420
    },
    {
      "epoch": 0.6719367588932806,
      "grad_norm": 0.23163548111915588,
      "learning_rate": 0.00013301737756714062,
      "loss": 0.03711399435997009,
      "step": 425
    },
    {
      "epoch": 0.6798418972332015,
      "grad_norm": 0.11196305602788925,
      "learning_rate": 0.00013222748815165878,
      "loss": 0.02715601325035095,
      "step": 430
    },
    {
      "epoch": 0.6877470355731226,
      "grad_norm": 0.0830475464463234,
      "learning_rate": 0.00013143759873617694,
      "loss": 0.027084299921989442,
      "step": 435
    },
    {
      "epoch": 0.6956521739130435,
      "grad_norm": 0.19524839520454407,
      "learning_rate": 0.0001306477093206951,
      "loss": 0.033523717522621156,
      "step": 440
    },
    {
      "epoch": 0.7035573122529645,
      "grad_norm": 0.1647607833147049,
      "learning_rate": 0.0001298578199052133,
      "loss": 0.03461429178714752,
      "step": 445
    },
    {
      "epoch": 0.7114624505928854,
      "grad_norm": 0.2741347551345825,
      "learning_rate": 0.00012906793048973145,
      "loss": 0.030341890454292298,
      "step": 450
    },
    {
      "epoch": 0.7193675889328063,
      "grad_norm": 0.22056451439857483,
      "learning_rate": 0.0001282780410742496,
      "loss": 0.03220893442630768,
      "step": 455
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.18708761036396027,
      "learning_rate": 0.00012748815165876778,
      "loss": 0.030454567074775694,
      "step": 460
    },
    {
      "epoch": 0.7351778656126482,
      "grad_norm": 0.1458652913570404,
      "learning_rate": 0.00012669826224328594,
      "loss": 0.022966325283050537,
      "step": 465
    },
    {
      "epoch": 0.7430830039525692,
      "grad_norm": 0.3877377510070801,
      "learning_rate": 0.00012590837282780413,
      "loss": 0.0366209089756012,
      "step": 470
    },
    {
      "epoch": 0.7509881422924901,
      "grad_norm": 0.3409236967563629,
      "learning_rate": 0.0001251184834123223,
      "loss": 0.04442314505577087,
      "step": 475
    },
    {
      "epoch": 0.758893280632411,
      "grad_norm": 0.16259066760540009,
      "learning_rate": 0.00012432859399684045,
      "loss": 0.032993391156196594,
      "step": 480
    },
    {
      "epoch": 0.766798418972332,
      "grad_norm": 0.14550919830799103,
      "learning_rate": 0.0001235387045813586,
      "loss": 0.04922538995742798,
      "step": 485
    },
    {
      "epoch": 0.7747035573122529,
      "grad_norm": 0.1469896286725998,
      "learning_rate": 0.00012274881516587677,
      "loss": 0.03784522116184234,
      "step": 490
    },
    {
      "epoch": 0.782608695652174,
      "grad_norm": 0.15060710906982422,
      "learning_rate": 0.00012195892575039496,
      "loss": 0.04871497452259064,
      "step": 495
    },
    {
      "epoch": 0.7905138339920948,
      "grad_norm": 0.10707800835371017,
      "learning_rate": 0.00012116903633491312,
      "loss": 0.02841663658618927,
      "step": 500
    },
    {
      "epoch": 0.7984189723320159,
      "grad_norm": 0.18339617550373077,
      "learning_rate": 0.00012037914691943129,
      "loss": 0.024705566465854645,
      "step": 505
    },
    {
      "epoch": 0.8063241106719368,
      "grad_norm": 0.29992640018463135,
      "learning_rate": 0.00011958925750394945,
      "loss": 0.0354970246553421,
      "step": 510
    },
    {
      "epoch": 0.8142292490118577,
      "grad_norm": 0.12703944742679596,
      "learning_rate": 0.00011879936808846761,
      "loss": 0.030265194177627564,
      "step": 515
    },
    {
      "epoch": 0.8221343873517787,
      "grad_norm": 0.4755694270133972,
      "learning_rate": 0.0001180094786729858,
      "loss": 0.03424926698207855,
      "step": 520
    },
    {
      "epoch": 0.8300395256916996,
      "grad_norm": 0.22839921712875366,
      "learning_rate": 0.00011721958925750396,
      "loss": 0.035613059997558594,
      "step": 525
    },
    {
      "epoch": 0.8379446640316206,
      "grad_norm": 0.19965742528438568,
      "learning_rate": 0.00011642969984202212,
      "loss": 0.032441169023513794,
      "step": 530
    },
    {
      "epoch": 0.8458498023715415,
      "grad_norm": 0.13590139150619507,
      "learning_rate": 0.00011563981042654028,
      "loss": 0.02755305767059326,
      "step": 535
    },
    {
      "epoch": 0.8537549407114624,
      "grad_norm": 0.705162525177002,
      "learning_rate": 0.00011484992101105845,
      "loss": 0.03638884723186493,
      "step": 540
    },
    {
      "epoch": 0.8616600790513834,
      "grad_norm": 0.2472056746482849,
      "learning_rate": 0.00011406003159557663,
      "loss": 0.029143399000167845,
      "step": 545
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 0.300169974565506,
      "learning_rate": 0.0001132701421800948,
      "loss": 0.03571694195270538,
      "step": 550
    },
    {
      "epoch": 0.8774703557312253,
      "grad_norm": 0.18986958265304565,
      "learning_rate": 0.00011248025276461296,
      "loss": 0.03161238431930542,
      "step": 555
    },
    {
      "epoch": 0.8853754940711462,
      "grad_norm": 0.20449358224868774,
      "learning_rate": 0.00011169036334913112,
      "loss": 0.028545519709587096,
      "step": 560
    },
    {
      "epoch": 0.8932806324110671,
      "grad_norm": 0.4060944616794586,
      "learning_rate": 0.00011090047393364928,
      "loss": 0.04297974109649658,
      "step": 565
    },
    {
      "epoch": 0.9011857707509882,
      "grad_norm": 0.20086570084095,
      "learning_rate": 0.00011011058451816747,
      "loss": 0.04026257395744324,
      "step": 570
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.21138064563274384,
      "learning_rate": 0.00010932069510268563,
      "loss": 0.0330931156873703,
      "step": 575
    },
    {
      "epoch": 0.9169960474308301,
      "grad_norm": 0.28250572085380554,
      "learning_rate": 0.0001085308056872038,
      "loss": 0.04564785063266754,
      "step": 580
    },
    {
      "epoch": 0.924901185770751,
      "grad_norm": 0.09428751468658447,
      "learning_rate": 0.00010774091627172196,
      "loss": 0.039422056078910826,
      "step": 585
    },
    {
      "epoch": 0.932806324110672,
      "grad_norm": 0.12124451249837875,
      "learning_rate": 0.00010695102685624012,
      "loss": 0.02727382481098175,
      "step": 590
    },
    {
      "epoch": 0.9407114624505929,
      "grad_norm": 0.15322023630142212,
      "learning_rate": 0.00010616113744075831,
      "loss": 0.038586610555648805,
      "step": 595
    },
    {
      "epoch": 0.9486166007905138,
      "grad_norm": 0.13494057953357697,
      "learning_rate": 0.00010537124802527647,
      "loss": 0.029403990507125853,
      "step": 600
    },
    {
      "epoch": 0.9565217391304348,
      "grad_norm": 0.1805979460477829,
      "learning_rate": 0.00010458135860979463,
      "loss": 0.03246787190437317,
      "step": 605
    },
    {
      "epoch": 0.9644268774703557,
      "grad_norm": 0.15115194022655487,
      "learning_rate": 0.00010379146919431279,
      "loss": 0.04212413430213928,
      "step": 610
    },
    {
      "epoch": 0.9723320158102767,
      "grad_norm": 0.3000054955482483,
      "learning_rate": 0.00010300157977883095,
      "loss": 0.02958311140537262,
      "step": 615
    },
    {
      "epoch": 0.9802371541501976,
      "grad_norm": 0.11541716754436493,
      "learning_rate": 0.00010221169036334914,
      "loss": 0.026830005645751952,
      "step": 620
    },
    {
      "epoch": 0.9881422924901185,
      "grad_norm": 0.31354743242263794,
      "learning_rate": 0.0001014218009478673,
      "loss": 0.04060990810394287,
      "step": 625
    },
    {
      "epoch": 0.9960474308300395,
      "grad_norm": 0.14256687462329865,
      "learning_rate": 0.00010063191153238547,
      "loss": 0.027988958358764648,
      "step": 630
    },
    {
      "epoch": 1.0031620553359684,
      "grad_norm": 0.1279018223285675,
      "learning_rate": 9.984202211690363e-05,
      "loss": 0.03264340162277222,
      "step": 635
    },
    {
      "epoch": 1.0110671936758893,
      "grad_norm": 0.13373367488384247,
      "learning_rate": 9.90521327014218e-05,
      "loss": 0.02540113627910614,
      "step": 640
    },
    {
      "epoch": 1.0189723320158102,
      "grad_norm": 0.23578080534934998,
      "learning_rate": 9.826224328593997e-05,
      "loss": 0.032197782397270204,
      "step": 645
    },
    {
      "epoch": 1.0268774703557313,
      "grad_norm": 0.1930210292339325,
      "learning_rate": 9.747235387045814e-05,
      "loss": 0.019389963150024413,
      "step": 650
    },
    {
      "epoch": 1.0347826086956522,
      "grad_norm": 0.2586020827293396,
      "learning_rate": 9.66824644549763e-05,
      "loss": 0.027432429790496825,
      "step": 655
    },
    {
      "epoch": 1.042687747035573,
      "grad_norm": 0.2777060270309448,
      "learning_rate": 9.589257503949447e-05,
      "loss": 0.028111490607261657,
      "step": 660
    },
    {
      "epoch": 1.050592885375494,
      "grad_norm": 0.2140793353319168,
      "learning_rate": 9.510268562401264e-05,
      "loss": 0.027586475014686584,
      "step": 665
    },
    {
      "epoch": 1.0584980237154151,
      "grad_norm": 0.15072056651115417,
      "learning_rate": 9.43127962085308e-05,
      "loss": 0.022282786667346954,
      "step": 670
    },
    {
      "epoch": 1.066403162055336,
      "grad_norm": 0.26284143328666687,
      "learning_rate": 9.352290679304898e-05,
      "loss": 0.0418301522731781,
      "step": 675
    },
    {
      "epoch": 1.074308300395257,
      "grad_norm": 0.11405179649591446,
      "learning_rate": 9.273301737756714e-05,
      "loss": 0.026996171474456786,
      "step": 680
    },
    {
      "epoch": 1.0822134387351778,
      "grad_norm": 0.10356621444225311,
      "learning_rate": 9.19431279620853e-05,
      "loss": 0.026044899225234987,
      "step": 685
    },
    {
      "epoch": 1.0901185770750987,
      "grad_norm": 0.32241159677505493,
      "learning_rate": 9.115323854660348e-05,
      "loss": 0.02852148413658142,
      "step": 690
    },
    {
      "epoch": 1.0980237154150199,
      "grad_norm": 0.45742568373680115,
      "learning_rate": 9.036334913112164e-05,
      "loss": 0.03570656478404999,
      "step": 695
    },
    {
      "epoch": 1.1059288537549408,
      "grad_norm": 0.16293948888778687,
      "learning_rate": 8.957345971563981e-05,
      "loss": 0.025030675530433654,
      "step": 700
    },
    {
      "epoch": 1.1138339920948617,
      "grad_norm": 0.0852792039513588,
      "learning_rate": 8.878357030015798e-05,
      "loss": 0.017969736456871034,
      "step": 705
    },
    {
      "epoch": 1.1217391304347826,
      "grad_norm": 0.2311236709356308,
      "learning_rate": 8.799368088467614e-05,
      "loss": 0.0249122753739357,
      "step": 710
    },
    {
      "epoch": 1.1296442687747035,
      "grad_norm": 0.2313305139541626,
      "learning_rate": 8.720379146919431e-05,
      "loss": 0.023807652294635773,
      "step": 715
    },
    {
      "epoch": 1.1375494071146246,
      "grad_norm": 0.13542281091213226,
      "learning_rate": 8.641390205371248e-05,
      "loss": 0.028155753016471864,
      "step": 720
    },
    {
      "epoch": 1.1454545454545455,
      "grad_norm": 0.4158993363380432,
      "learning_rate": 8.562401263823065e-05,
      "loss": 0.03010842204093933,
      "step": 725
    },
    {
      "epoch": 1.1533596837944664,
      "grad_norm": 0.19011123478412628,
      "learning_rate": 8.483412322274881e-05,
      "loss": 0.03321767449378967,
      "step": 730
    },
    {
      "epoch": 1.1612648221343873,
      "grad_norm": 0.13528752326965332,
      "learning_rate": 8.404423380726699e-05,
      "loss": 0.02816251516342163,
      "step": 735
    },
    {
      "epoch": 1.1691699604743082,
      "grad_norm": 0.21150588989257812,
      "learning_rate": 8.325434439178515e-05,
      "loss": 0.02879970371723175,
      "step": 740
    },
    {
      "epoch": 1.1770750988142293,
      "grad_norm": 0.17024262249469757,
      "learning_rate": 8.246445497630332e-05,
      "loss": 0.031139376759529113,
      "step": 745
    },
    {
      "epoch": 1.1849802371541502,
      "grad_norm": 0.18700863420963287,
      "learning_rate": 8.167456556082149e-05,
      "loss": 0.03445093035697937,
      "step": 750
    },
    {
      "epoch": 1.1928853754940711,
      "grad_norm": 0.09378049522638321,
      "learning_rate": 8.088467614533965e-05,
      "loss": 0.030979880690574647,
      "step": 755
    },
    {
      "epoch": 1.200790513833992,
      "grad_norm": 0.10371541976928711,
      "learning_rate": 8.009478672985782e-05,
      "loss": 0.026752352714538574,
      "step": 760
    },
    {
      "epoch": 1.208695652173913,
      "grad_norm": 0.12506705522537231,
      "learning_rate": 7.930489731437599e-05,
      "loss": 0.024311460554599762,
      "step": 765
    },
    {
      "epoch": 1.216600790513834,
      "grad_norm": 0.2918132245540619,
      "learning_rate": 7.851500789889416e-05,
      "loss": 0.030068397521972656,
      "step": 770
    },
    {
      "epoch": 1.224505928853755,
      "grad_norm": 0.13819359242916107,
      "learning_rate": 7.772511848341232e-05,
      "loss": 0.02760131061077118,
      "step": 775
    },
    {
      "epoch": 1.2324110671936759,
      "grad_norm": 0.1774282455444336,
      "learning_rate": 7.69352290679305e-05,
      "loss": 0.03288972675800324,
      "step": 780
    },
    {
      "epoch": 1.2403162055335968,
      "grad_norm": 0.27049848437309265,
      "learning_rate": 7.614533965244866e-05,
      "loss": 0.0317463219165802,
      "step": 785
    },
    {
      "epoch": 1.2482213438735177,
      "grad_norm": 0.13078685104846954,
      "learning_rate": 7.535545023696684e-05,
      "loss": 0.01935163289308548,
      "step": 790
    },
    {
      "epoch": 1.2561264822134388,
      "grad_norm": 0.1510840505361557,
      "learning_rate": 7.4565560821485e-05,
      "loss": 0.027038556337356568,
      "step": 795
    },
    {
      "epoch": 1.2640316205533597,
      "grad_norm": 0.12462210655212402,
      "learning_rate": 7.377567140600317e-05,
      "loss": 0.022488293051719666,
      "step": 800
    },
    {
      "epoch": 1.2719367588932806,
      "grad_norm": 0.2583881914615631,
      "learning_rate": 7.298578199052133e-05,
      "loss": 0.029116451740264893,
      "step": 805
    },
    {
      "epoch": 1.2798418972332015,
      "grad_norm": 0.32093125581741333,
      "learning_rate": 7.219589257503951e-05,
      "loss": 0.03329278528690338,
      "step": 810
    },
    {
      "epoch": 1.2877470355731226,
      "grad_norm": 0.33992961049079895,
      "learning_rate": 7.140600315955767e-05,
      "loss": 0.026922634243965148,
      "step": 815
    },
    {
      "epoch": 1.2956521739130435,
      "grad_norm": 0.2556058168411255,
      "learning_rate": 7.061611374407583e-05,
      "loss": 0.0240438848733902,
      "step": 820
    },
    {
      "epoch": 1.3035573122529645,
      "grad_norm": 0.140956312417984,
      "learning_rate": 6.982622432859401e-05,
      "loss": 0.02204260528087616,
      "step": 825
    },
    {
      "epoch": 1.3114624505928854,
      "grad_norm": 0.26354745030403137,
      "learning_rate": 6.903633491311217e-05,
      "loss": 0.027074044942855834,
      "step": 830
    },
    {
      "epoch": 1.3193675889328063,
      "grad_norm": 0.1549350768327713,
      "learning_rate": 6.824644549763035e-05,
      "loss": 0.030218863487243654,
      "step": 835
    },
    {
      "epoch": 1.3272727272727272,
      "grad_norm": 0.3250249922275543,
      "learning_rate": 6.745655608214851e-05,
      "loss": 0.0323047935962677,
      "step": 840
    },
    {
      "epoch": 1.3351778656126483,
      "grad_norm": 0.15636257827281952,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.028157037496566773,
      "step": 845
    },
    {
      "epoch": 1.3430830039525692,
      "grad_norm": 0.24536046385765076,
      "learning_rate": 6.587677725118485e-05,
      "loss": 0.039545825123786925,
      "step": 850
    },
    {
      "epoch": 1.35098814229249,
      "grad_norm": 0.21744322776794434,
      "learning_rate": 6.508688783570301e-05,
      "loss": 0.01926095336675644,
      "step": 855
    },
    {
      "epoch": 1.358893280632411,
      "grad_norm": 0.3079163134098053,
      "learning_rate": 6.429699842022118e-05,
      "loss": 0.031944838166236875,
      "step": 860
    },
    {
      "epoch": 1.3667984189723321,
      "grad_norm": 0.5792288184165955,
      "learning_rate": 6.350710900473934e-05,
      "loss": 0.036422842741012575,
      "step": 865
    },
    {
      "epoch": 1.374703557312253,
      "grad_norm": 0.18825426697731018,
      "learning_rate": 6.27172195892575e-05,
      "loss": 0.023360159993171693,
      "step": 870
    },
    {
      "epoch": 1.382608695652174,
      "grad_norm": 0.15373866260051727,
      "learning_rate": 6.192733017377568e-05,
      "loss": 0.03461389243602753,
      "step": 875
    },
    {
      "epoch": 1.3905138339920948,
      "grad_norm": 0.1236257553100586,
      "learning_rate": 6.113744075829384e-05,
      "loss": 0.02693447768688202,
      "step": 880
    },
    {
      "epoch": 1.3984189723320157,
      "grad_norm": 0.1737414449453354,
      "learning_rate": 6.034755134281201e-05,
      "loss": 0.028666308522224425,
      "step": 885
    },
    {
      "epoch": 1.4063241106719366,
      "grad_norm": 0.21450752019882202,
      "learning_rate": 5.955766192733018e-05,
      "loss": 0.032833141088485715,
      "step": 890
    },
    {
      "epoch": 1.4142292490118578,
      "grad_norm": 0.18014955520629883,
      "learning_rate": 5.876777251184834e-05,
      "loss": 0.02754313051700592,
      "step": 895
    },
    {
      "epoch": 1.4221343873517787,
      "grad_norm": 0.16805581748485565,
      "learning_rate": 5.797788309636651e-05,
      "loss": 0.020001547038555147,
      "step": 900
    },
    {
      "epoch": 1.4300395256916996,
      "grad_norm": 0.1497848629951477,
      "learning_rate": 5.718799368088468e-05,
      "loss": 0.026247042417526244,
      "step": 905
    },
    {
      "epoch": 1.4379446640316207,
      "grad_norm": 0.11485550552606583,
      "learning_rate": 5.639810426540285e-05,
      "loss": 0.025514575839042663,
      "step": 910
    },
    {
      "epoch": 1.4458498023715416,
      "grad_norm": 0.3395633101463318,
      "learning_rate": 5.560821484992102e-05,
      "loss": 0.024022576212882996,
      "step": 915
    },
    {
      "epoch": 1.4537549407114625,
      "grad_norm": 0.2780166268348694,
      "learning_rate": 5.481832543443918e-05,
      "loss": 0.026301485300064088,
      "step": 920
    },
    {
      "epoch": 1.4616600790513834,
      "grad_norm": 0.4303275942802429,
      "learning_rate": 5.4028436018957354e-05,
      "loss": 0.03279286026954651,
      "step": 925
    },
    {
      "epoch": 1.4695652173913043,
      "grad_norm": 0.2073795199394226,
      "learning_rate": 5.3238546603475516e-05,
      "loss": 0.024443019926548005,
      "step": 930
    },
    {
      "epoch": 1.4774703557312252,
      "grad_norm": 0.6732660531997681,
      "learning_rate": 5.244865718799369e-05,
      "loss": 0.033600634336471556,
      "step": 935
    },
    {
      "epoch": 1.485375494071146,
      "grad_norm": 0.39328768849372864,
      "learning_rate": 5.165876777251185e-05,
      "loss": 0.03240462839603424,
      "step": 940
    },
    {
      "epoch": 1.4932806324110672,
      "grad_norm": 0.17230291664600372,
      "learning_rate": 5.0868878357030015e-05,
      "loss": 0.03093341588973999,
      "step": 945
    },
    {
      "epoch": 1.5011857707509881,
      "grad_norm": 0.29362860321998596,
      "learning_rate": 5.007898894154819e-05,
      "loss": 0.02297430783510208,
      "step": 950
    },
    {
      "epoch": 1.509090909090909,
      "grad_norm": 0.1910446435213089,
      "learning_rate": 4.928909952606635e-05,
      "loss": 0.03064105808734894,
      "step": 955
    },
    {
      "epoch": 1.5169960474308302,
      "grad_norm": 0.28715845942497253,
      "learning_rate": 4.849921011058452e-05,
      "loss": 0.024530869722366334,
      "step": 960
    },
    {
      "epoch": 1.524901185770751,
      "grad_norm": 0.2624508738517761,
      "learning_rate": 4.770932069510269e-05,
      "loss": 0.026721322536468507,
      "step": 965
    },
    {
      "epoch": 1.532806324110672,
      "grad_norm": 0.357749879360199,
      "learning_rate": 4.691943127962086e-05,
      "loss": 0.028246894478797913,
      "step": 970
    },
    {
      "epoch": 1.5407114624505929,
      "grad_norm": 0.2578469216823578,
      "learning_rate": 4.6129541864139027e-05,
      "loss": 0.02935582399368286,
      "step": 975
    },
    {
      "epoch": 1.5486166007905138,
      "grad_norm": 0.3196616470813751,
      "learning_rate": 4.533965244865719e-05,
      "loss": 0.017465047538280487,
      "step": 980
    },
    {
      "epoch": 1.5565217391304347,
      "grad_norm": 0.25240039825439453,
      "learning_rate": 4.454976303317536e-05,
      "loss": 0.021775716543197633,
      "step": 985
    },
    {
      "epoch": 1.5644268774703556,
      "grad_norm": 0.2092476338148117,
      "learning_rate": 4.3759873617693526e-05,
      "loss": 0.02117091566324234,
      "step": 990
    },
    {
      "epoch": 1.5723320158102767,
      "grad_norm": 0.26376646757125854,
      "learning_rate": 4.2969984202211694e-05,
      "loss": 0.02211945354938507,
      "step": 995
    },
    {
      "epoch": 1.5802371541501976,
      "grad_norm": 0.3229852318763733,
      "learning_rate": 4.218009478672986e-05,
      "loss": 0.02165742814540863,
      "step": 1000
    },
    {
      "epoch": 1.5881422924901187,
      "grad_norm": 0.38538622856140137,
      "learning_rate": 4.1390205371248025e-05,
      "loss": 0.03688770532608032,
      "step": 1005
    },
    {
      "epoch": 1.5960474308300396,
      "grad_norm": 0.12444254010915756,
      "learning_rate": 4.060031595576619e-05,
      "loss": 0.029797789454460145,
      "step": 1010
    },
    {
      "epoch": 1.6039525691699605,
      "grad_norm": 0.2925982177257538,
      "learning_rate": 3.981042654028436e-05,
      "loss": 0.02609897553920746,
      "step": 1015
    },
    {
      "epoch": 1.6118577075098814,
      "grad_norm": 0.5516015291213989,
      "learning_rate": 3.902053712480253e-05,
      "loss": 0.04365030825138092,
      "step": 1020
    },
    {
      "epoch": 1.6197628458498023,
      "grad_norm": 0.28329846262931824,
      "learning_rate": 3.82306477093207e-05,
      "loss": 0.026367220282554626,
      "step": 1025
    },
    {
      "epoch": 1.6276679841897232,
      "grad_norm": 0.25014129281044006,
      "learning_rate": 3.744075829383886e-05,
      "loss": 0.02157619148492813,
      "step": 1030
    },
    {
      "epoch": 1.6355731225296442,
      "grad_norm": 0.17009703814983368,
      "learning_rate": 3.665086887835703e-05,
      "loss": 0.021255302429199218,
      "step": 1035
    },
    {
      "epoch": 1.643478260869565,
      "grad_norm": 0.18074770271778107,
      "learning_rate": 3.58609794628752e-05,
      "loss": 0.023342445492744446,
      "step": 1040
    },
    {
      "epoch": 1.6513833992094862,
      "grad_norm": 0.20974715054035187,
      "learning_rate": 3.507109004739337e-05,
      "loss": 0.01858111619949341,
      "step": 1045
    },
    {
      "epoch": 1.659288537549407,
      "grad_norm": 0.13896572589874268,
      "learning_rate": 3.4281200631911535e-05,
      "loss": 0.01670115292072296,
      "step": 1050
    },
    {
      "epoch": 1.6671936758893282,
      "grad_norm": 0.7247277498245239,
      "learning_rate": 3.34913112164297e-05,
      "loss": 0.024269095063209532,
      "step": 1055
    },
    {
      "epoch": 1.6750988142292491,
      "grad_norm": 0.326495885848999,
      "learning_rate": 3.2701421800947866e-05,
      "loss": 0.027763289213180543,
      "step": 1060
    },
    {
      "epoch": 1.68300395256917,
      "grad_norm": 0.264034628868103,
      "learning_rate": 3.1911532385466034e-05,
      "loss": 0.026523405313491823,
      "step": 1065
    },
    {
      "epoch": 1.690909090909091,
      "grad_norm": 0.39051833748817444,
      "learning_rate": 3.11216429699842e-05,
      "loss": 0.019228774309158325,
      "step": 1070
    },
    {
      "epoch": 1.6988142292490118,
      "grad_norm": 0.2692244350910187,
      "learning_rate": 3.0331753554502375e-05,
      "loss": 0.02024847865104675,
      "step": 1075
    },
    {
      "epoch": 1.7067193675889327,
      "grad_norm": 0.4079023003578186,
      "learning_rate": 2.9541864139020537e-05,
      "loss": 0.03832553923130035,
      "step": 1080
    },
    {
      "epoch": 1.7146245059288536,
      "grad_norm": 0.091139055788517,
      "learning_rate": 2.8751974723538705e-05,
      "loss": 0.01793159544467926,
      "step": 1085
    },
    {
      "epoch": 1.7225296442687748,
      "grad_norm": 0.10134904086589813,
      "learning_rate": 2.7962085308056874e-05,
      "loss": 0.017810808122158052,
      "step": 1090
    },
    {
      "epoch": 1.7304347826086957,
      "grad_norm": 0.14640779793262482,
      "learning_rate": 2.7172195892575043e-05,
      "loss": 0.01706620305776596,
      "step": 1095
    },
    {
      "epoch": 1.7383399209486166,
      "grad_norm": 0.36270618438720703,
      "learning_rate": 2.638230647709321e-05,
      "loss": 0.01808219403028488,
      "step": 1100
    },
    {
      "epoch": 1.7462450592885377,
      "grad_norm": 0.510646641254425,
      "learning_rate": 2.5592417061611373e-05,
      "loss": 0.03235427141189575,
      "step": 1105
    },
    {
      "epoch": 1.7541501976284586,
      "grad_norm": 0.6470493078231812,
      "learning_rate": 2.480252764612954e-05,
      "loss": 0.027404117584228515,
      "step": 1110
    },
    {
      "epoch": 1.7620553359683795,
      "grad_norm": 0.16468720138072968,
      "learning_rate": 2.401263823064771e-05,
      "loss": 0.026439195871353148,
      "step": 1115
    },
    {
      "epoch": 1.7699604743083004,
      "grad_norm": 0.2764127850532532,
      "learning_rate": 2.322274881516588e-05,
      "loss": 0.035968080163002014,
      "step": 1120
    },
    {
      "epoch": 1.7778656126482213,
      "grad_norm": 0.28152668476104736,
      "learning_rate": 2.2432859399684044e-05,
      "loss": 0.021218731999397278,
      "step": 1125
    },
    {
      "epoch": 1.7857707509881422,
      "grad_norm": 0.3192931115627289,
      "learning_rate": 2.1642969984202213e-05,
      "loss": 0.02865079939365387,
      "step": 1130
    },
    {
      "epoch": 1.793675889328063,
      "grad_norm": 0.13518910109996796,
      "learning_rate": 2.0853080568720378e-05,
      "loss": 0.017810915410518647,
      "step": 1135
    },
    {
      "epoch": 1.8015810276679842,
      "grad_norm": 0.34582874178886414,
      "learning_rate": 2.0063191153238547e-05,
      "loss": 0.020189496874809264,
      "step": 1140
    },
    {
      "epoch": 1.8094861660079051,
      "grad_norm": 0.2769288420677185,
      "learning_rate": 1.9273301737756715e-05,
      "loss": 0.029002615809440614,
      "step": 1145
    },
    {
      "epoch": 1.8173913043478263,
      "grad_norm": 0.47925400733947754,
      "learning_rate": 1.848341232227488e-05,
      "loss": 0.023514480888843538,
      "step": 1150
    },
    {
      "epoch": 1.8252964426877472,
      "grad_norm": 0.1271977722644806,
      "learning_rate": 1.769352290679305e-05,
      "loss": 0.02011231184005737,
      "step": 1155
    },
    {
      "epoch": 1.833201581027668,
      "grad_norm": 0.4072546362876892,
      "learning_rate": 1.6903633491311218e-05,
      "loss": 0.030403399467468263,
      "step": 1160
    },
    {
      "epoch": 1.841106719367589,
      "grad_norm": 0.3055180311203003,
      "learning_rate": 1.6113744075829386e-05,
      "loss": 0.02987173795700073,
      "step": 1165
    },
    {
      "epoch": 1.8490118577075099,
      "grad_norm": 0.2547559440135956,
      "learning_rate": 1.532385466034755e-05,
      "loss": 0.02212100327014923,
      "step": 1170
    },
    {
      "epoch": 1.8569169960474308,
      "grad_norm": 0.21211273968219757,
      "learning_rate": 1.4533965244865718e-05,
      "loss": 0.021875013411045075,
      "step": 1175
    },
    {
      "epoch": 1.8648221343873517,
      "grad_norm": 0.19742442667484283,
      "learning_rate": 1.3744075829383887e-05,
      "loss": 0.02210855633020401,
      "step": 1180
    },
    {
      "epoch": 1.8727272727272726,
      "grad_norm": 0.2860613763332367,
      "learning_rate": 1.2954186413902054e-05,
      "loss": 0.023570165038108826,
      "step": 1185
    },
    {
      "epoch": 1.8806324110671937,
      "grad_norm": 0.44927772879600525,
      "learning_rate": 1.216429699842022e-05,
      "loss": 0.027138009667396545,
      "step": 1190
    },
    {
      "epoch": 1.8885375494071146,
      "grad_norm": 0.2762649953365326,
      "learning_rate": 1.137440758293839e-05,
      "loss": 0.028670135140419006,
      "step": 1195
    },
    {
      "epoch": 1.8964426877470357,
      "grad_norm": 0.4845801591873169,
      "learning_rate": 1.0584518167456558e-05,
      "loss": 0.02700424790382385,
      "step": 1200
    }
  ],
  "logging_steps": 5,
  "max_steps": 1266,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 20,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4909486035977274e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
