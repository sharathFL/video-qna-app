services:
  vlm:
    build: .
    container_name: vlm_safe_unsafe
    working_dir: /workspace
    # 8080 = port hub (what runs where); 8082 = video inference
    command: ["sh", "-c", "python src/port_hub.py --port 8080 & exec python src/serve_video_inference.py --checkpoint /workspace/outputs/checkpoint-2056 --port 8082"]
    volumes:
      - ./data:/workspace/data:rw
      - ./src:/workspace/src:rw
      - ./outputs:/workspace/outputs:rw
      - ./cache/hf:/workspace/.cache/huggingface:rw
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/workspace/.cache/huggingface
      - TRANSFORMERS_CACHE=/workspace/.cache/huggingface
    shm_size: "2gb"
    stdin_open: true
    tty: true
    ports:
      - "8080:8080"
      - "8082:8082"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # YouTube inference on 8083: paste URL, embed video, run inference (no download)
  youtube:
    build: .
    container_name: vlm_youtube
    working_dir: /workspace
    command: ["python", "src/serve_youtube_inference.py", "--checkpoint", "/workspace/outputs/checkpoint-2056", "--port", "8083"]
    volumes:
      - ./data:/workspace/data:rw
      - ./src:/workspace/src:rw
      - ./outputs:/workspace/outputs:rw
      - ./cache/hf:/workspace/.cache/huggingface:rw
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/workspace/.cache/huggingface
      - TRANSFORMERS_CACHE=/workspace/.cache/huggingface
    shm_size: "2gb"
    stdin_open: true
    tty: true
    ports:
      - "8083:8083"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # VLM QA on 8087: DINOv3 (default) or SmolVLM2/LLaVA. DINOv3 = vision backbone only (no QA); use /dinov3_features. VLMs: ask questions, attention map (SmolVLM2).
  vlm_qa:
    build: .
    container_name: vlm_qa
    working_dir: /workspace
    command: ["python", "src/serve_vlm_qa.py", "--port", "8087", "--model", "dinov3"]
    volumes:
      - ./src:/workspace/src:rw
      - ./cache/hf:/workspace/.cache/huggingface:rw
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/workspace/.cache/huggingface
      - TRANSFORMERS_CACHE=/workspace/.cache/huggingface
      # Gated DINOv3: accept license at https://huggingface.co/facebook/dinov3-vits16-pretrain-lvd1689m then: HF_TOKEN=hf_xxx docker compose up -d vlm_qa
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    shm_size: "2gb"
    stdin_open: true
    tty: true
    ports:
      - "8087:8087"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
