services:
  vlm:
    build: .
    container_name: vlm_safe_unsafe
    working_dir: /workspace
    # Run video inference on 8082 (model loaded once at startup).
    command: ["python", "src/serve_video_inference.py", "--checkpoint", "/workspace/outputs/checkpoint-2056", "--port", "8082"]
    volumes:
      - ./data:/workspace/data:rw
      - ./src:/workspace/src:rw
      - ./outputs:/workspace/outputs:rw
      - ./cache/hf:/workspace/.cache/huggingface:rw
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/workspace/.cache/huggingface
      - TRANSFORMERS_CACHE=/workspace/.cache/huggingface
    shm_size: "2gb"
    stdin_open: true
    tty: true
    ports:
      - "8080:8080"
      - "8082:8082"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # YouTube inference: embed video, run inference (no download), checkpoint 2056
  youtube:
    build: .
    container_name: vlm_youtube
    working_dir: /workspace
    command: ["python", "src/serve_youtube_inference.py", "--checkpoint", "/workspace/outputs/checkpoint-2056", "--port", "8083"]
    volumes:
      - ./data:/workspace/data:rw
      - ./src:/workspace/src:rw
      - ./outputs:/workspace/outputs:rw
      - ./cache/hf:/workspace/.cache/huggingface:rw
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/workspace/.cache/huggingface
      - TRANSFORMERS_CACHE=/workspace/.cache/huggingface
    shm_size: "2gb"
    stdin_open: true
    tty: true
    ports:
      - "8083:8083"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
