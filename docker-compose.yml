services:
  vlm:
    build: .
    container_name: vlm_safe_unsafe
    working_dir: /workspace
    # 8080 = port hub (what runs where); 8082 = video inference
    command: ["sh", "-c", "python src/port_hub.py --port 8080 & exec python src/serve_video_inference.py --checkpoint /workspace/outputs/checkpoint-2056 --port 8082"]
    volumes:
      - ./data:/workspace/data:rw
      - ./src:/workspace/src:rw
      - ./outputs:/workspace/outputs:rw
      - ./cache/hf:/workspace/.cache/huggingface:rw
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/workspace/.cache/huggingface
      - TRANSFORMERS_CACHE=/workspace/.cache/huggingface
    shm_size: "2gb"
    stdin_open: true
    tty: true
    ports:
      - "8080:8080"
      - "8082:8082"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # YouTube inference on 8083: paste URL, embed video, run inference (no download)
  youtube:
    build: .
    container_name: vlm_youtube
    working_dir: /workspace
    command: ["python", "src/serve_youtube_inference.py", "--checkpoint", "/workspace/outputs/checkpoint-2056", "--port", "8083"]
    volumes:
      - ./data:/workspace/data:rw
      - ./src:/workspace/src:rw
      - ./outputs:/workspace/outputs:rw
      - ./cache/hf:/workspace/.cache/huggingface:rw
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/workspace/.cache/huggingface
      - TRANSFORMERS_CACHE=/workspace/.cache/huggingface
    shm_size: "2gb"
    stdin_open: true
    tty: true
    ports:
      - "8083:8083"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
